{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fe356a9",
   "metadata": {},
   "source": [
    "# 01_dataset_schema_exploration.ipynb\n",
    "# ðŸ“¡ Satellite Communications Dataset â€“ Schema Exploration\n",
    "\n",
    "\"\"\"\n",
    "Overview:\n",
    "This notebook explores the SatNOGS satellite communications dataset schema.\n",
    "Goal: Understand the structure of all tables, their columns, and row counts,\n",
    "so we can later prepare the dataset for machine learning tasks.\n",
    "\n",
    "Objectives:\n",
    "- Inspect each table in the database (`satnogs`)\n",
    "- Document column names and row counts\n",
    "- Preview sample rows for context\n",
    "- Identify relationships between tables\n",
    "- Save schema summaries for reuse\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3796519e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (2.0.44)\n",
      "Requirement already satisfied: pandas in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: pymysql in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: tabulate in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: greenlet>=1 in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (from sqlalchemy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (from sqlalchemy) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: tabulate in d:\\elo 2\\satnogs_project\\.venv\\lib\\site-packages (0.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "SQLAlchemy version: 2.0.44\n",
      "Pandas version: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "# --- Install dependencies (only needed once in environment) ---\n",
    "%pip install sqlalchemy pandas pymysql tabulate\n",
    "%pip install tabulate\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import importlib\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "print(\"SQLAlchemy version:\", sqlalchemy.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c8f18b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Database connection ---\n",
    "DB_USER = \"root\"\n",
    "DB_PASSWORD = \"123456789\"\n",
    "DB_HOST = \"127.0.0.1\"\n",
    "DB_PORT = \"3306\"\n",
    "DB_NAME = \"satnogs\"\n",
    "\n",
    "engine = create_engine(f\"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "inspector = inspect(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a9ee1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in database: ['base_antenna', 'base_antennatype', 'base_frequencyrange', 'base_launch', 'base_mode', 'base_observation', 'base_operator', 'base_satellite', 'base_satelliteentry', 'base_satelliteidentifier', 'base_station', 'base_stationstatuslog', 'base_stationtype', 'base_telemetry', 'base_transmitterentry']\n"
     ]
    }
   ],
   "source": [
    "# --- List all tables ---\n",
    "tables = inspector.get_table_names()\n",
    "print(\"Tables in database:\", tables)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba9353aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Running: Dataset timeline\n",
      "âœ… Retrieved 1 rows in 30.95s\n",
      "    first_observation    last_observation\n",
      "0 2015-10-12 15:13:16 2025-11-12 09:59:41\n"
     ]
    }
   ],
   "source": [
    "# --- Check the dataset timeline ---\n",
    "\n",
    "import time\n",
    "\n",
    "def run_query(sql, engine, desc=\"Query\"):\n",
    "    \"\"\"Run SQL query and return DataFrame with logging.\"\"\"\n",
    "    print(f\"\\nðŸ” Running: {desc}\")\n",
    "    start = time.time()\n",
    "    df = pd.read_sql_query(sql, engine)\n",
    "    print(f\"âœ… Retrieved {len(df):,} rows in {time.time()-start:.2f}s\")\n",
    "    return df\n",
    "timeline_sql = \"\"\"\n",
    "SELECT MIN(start) AS first_observation,\n",
    "       MAX(start) AS last_observation\n",
    "FROM base_observation\n",
    "WHERE start IS NOT NULL\n",
    "\"\"\"\n",
    "timeline = run_query(timeline_sql, engine, \"Dataset timeline\")\n",
    "print(timeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c53619ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper function to explore a table ---\n",
    "def explore_table(table_name, preview_rows=5):\n",
    "    \"\"\"Return schema summary for a given table.\"\"\"\n",
    "    cols = [c['name'] for c in inspector.get_columns(table_name)]\n",
    "    count = pd.read_sql(f\"SELECT COUNT(*) AS total_rows FROM {table_name}\", engine)['total_rows'][0]\n",
    "    df_preview = pd.read_sql(f\"SELECT * FROM {table_name} LIMIT {preview_rows}\", engine)\n",
    "    \n",
    "    summary = {\n",
    "        \"table\": table_name,\n",
    "        \"columns\": cols,\n",
    "        \"row_count\": count,\n",
    "        \"preview\": df_preview\n",
    "    }\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c283efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== base_antenna ===\n",
      "Columns: ['id', 'antenna_type_id', 'station_id']\n",
      "Row count: 4437\n",
      "   id  antenna_type_id  station_id\n",
      "0   1               12         256\n",
      "1   2               12        1536\n",
      "2   7               10         771\n",
      "3   8               13        1285\n",
      "4   9               15           6\n",
      "âœ… Finished processing base_antenna\n",
      "\n",
      "=== base_antennatype ===\n",
      "Columns: ['id', 'name']\n",
      "Row count: 17\n",
      "   id          name\n",
      "0   6    Cross Yagi\n",
      "1   1        Dipole\n",
      "2   3       Discone\n",
      "3  12     Eggbeater\n",
      "4   4  Ground Plane\n",
      "âœ… Finished processing base_antennatype\n",
      "\n",
      "=== base_frequencyrange ===\n",
      "Columns: ['id', 'min_frequency', 'max_frequency', 'antenna_id']\n",
      "Row count: 5311\n",
      "   id  min_frequency  max_frequency  antenna_id\n",
      "0   1      135000000      148000000           1\n",
      "1   2      430000000      440000000           2\n",
      "2   7      135000000      148000000           7\n",
      "3   8      430000000      440000000           8\n",
      "4   9      400000000      470000000           9\n",
      "âœ… Finished processing base_frequencyrange\n",
      "\n",
      "=== base_launch ===\n",
      "Columns: ['id', 'name', 'forum_thread_url', 'created', 'created_by_id']\n",
      "Row count: 0\n",
      "Empty DataFrame\n",
      "Columns: [id, name, forum_thread_url, created, created_by_id]\n",
      "Index: []\n",
      "âœ… Finished processing base_launch\n",
      "\n",
      "=== base_mode ===\n",
      "Columns: ['id', 'name']\n",
      "Row count: 56\n",
      "   id          name\n",
      "0  90          4FSK\n",
      "1  49          AFSK\n",
      "2  78  AFSK TUBiX10\n",
      "3  17         AHRPT\n",
      "4  19            AM\n",
      "âœ… Finished processing base_mode\n",
      "\n",
      "=== base_observation ===\n",
      "Columns: ['id', 'start', 'end', 'author_id', 'ground_station_id', 'max_altitude', 'rise_azimuth', 'set_azimuth', 'waterfall_status_datetime', 'vetted_status', 'waterfall_status_user_id', 'archive_identifier', 'archive_url', 'archived', 'experimental', 'client_metadata', 'client_version', 'transmitter_baud', 'transmitter_created', 'transmitter_description', 'transmitter_downlink_drift', 'transmitter_downlink_high', 'transmitter_downlink_low', 'transmitter_invert', 'transmitter_type', 'transmitter_uplink_drift', 'transmitter_uplink_high', 'transmitter_uplink_low', 'transmitter_uuid', 'transmitter_mode', 'status', 'waterfall_status', 'tle_line_0', 'tle_line_1', 'tle_line_2', 'tle_source', 'tle_updated', 'station_alt', 'station_antennas', 'station_lat', 'station_lng', 'audio_zipped', 'payload', 'waterfall', 'center_frequency', 'transmitter_status', 'transmitter_unconfirmed', 'sat_id']\n",
      "Row count: 12546241\n",
      "   id               start                 end  author_id  ground_station_id  \\\n",
      "0  23 2015-10-12 15:13:16 2015-10-12 15:20:01        165                  2   \n",
      "1  25 2015-10-12 17:03:08 2015-10-12 17:15:37        165                  2   \n",
      "2  27 2015-10-12 17:30:27 2015-10-12 17:43:51        165                  2   \n",
      "3  28 2015-10-12 18:08:16 2015-10-12 18:21:54        165                  2   \n",
      "4  33 2015-10-13 17:08:24 2015-10-13 17:21:56        165                  2   \n",
      "\n",
      "  max_altitude rise_azimuth set_azimuth waterfall_status_datetime  \\\n",
      "0         None         None        None       2017-05-26 08:04:08   \n",
      "1         None         None        None       2017-05-26 08:18:15   \n",
      "2         None         None        None       2017-05-26 08:22:41   \n",
      "3         None         None        None       2017-07-22 11:14:03   \n",
      "4         None         None        None       2017-09-10 19:00:05   \n",
      "\n",
      "  vetted_status  ...  station_antennas station_lat station_lng  audio_zipped  \\\n",
      "0           bad  ...              None        None        None             1   \n",
      "1           bad  ...              None        None        None             1   \n",
      "2           bad  ...              None        None        None             1   \n",
      "3           bad  ...              None        None        None             1   \n",
      "4           bad  ...              None        None        None             1   \n",
      "\n",
      "   payload waterfall center_frequency  transmitter_status  \\\n",
      "0                                None                None   \n",
      "1                                None                None   \n",
      "2                                None                None   \n",
      "3                                None                None   \n",
      "4                                None                None   \n",
      "\n",
      "  transmitter_unconfirmed                    sat_id  \n",
      "0                    None  UTXU-4881-3195-9394-3367  \n",
      "1                    None  FVYN-9469-5031-2236-7972  \n",
      "2                    None  HXCH-9043-9893-2952-4877  \n",
      "3                    None  IRES-5964-9687-1982-0089  \n",
      "4                    None  ZRIM-9073-8711-5268-6171  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "âœ… Finished processing base_observation\n",
      "\n",
      "=== base_operator ===\n",
      "Columns: ['id', 'name', 'names', 'description', 'website']\n",
      "Row count: 6\n",
      "   id   name                                              names  \\\n",
      "0   1    UVG                 Universidad del Valle de Guatemala   \n",
      "1   2    LSF                             Libre Space Foundation   \n",
      "2   3    ESA                              European Space Agency   \n",
      "3   4   ISRO                 Indian Space Research Organisation   \n",
      "4   5  CIOMP  Changchun Institute of Optics, Fine Mechanics ...   \n",
      "\n",
      "                                         description  \\\n",
      "0                                                      \n",
      "1                                                      \n",
      "2                                                      \n",
      "3  The Indian Space Research Organisation is the ...   \n",
      "4                                                      \n",
      "\n",
      "                        website  \n",
      "0       https://www.uvg.edu.gt/  \n",
      "1           https://libre.space  \n",
      "2          https://www.esa.int/  \n",
      "3      https://www.isro.gov.in/  \n",
      "4  http://english.ciomp.cas.cn/  \n",
      "âœ… Finished processing base_operator\n",
      "\n",
      "=== base_satellite ===\n",
      "Columns: ['id', 'last_modified', 'associated_satellite_id', 'satellite_entry_id', 'satellite_identifier_id']\n",
      "Row count: 2903\n",
      "   id              last_modified associated_satellite_id  satellite_entry_id  \\\n",
      "0   1 2021-07-21 10:11:41.947596                    None                7538   \n",
      "1   2 2021-07-21 10:11:41.949944                    None                1843   \n",
      "2   3 2021-07-21 10:11:41.951757                    None                1836   \n",
      "3   4 2021-07-21 10:11:41.953531                    None                1930   \n",
      "4   5 2021-07-21 10:11:41.955319                    None                1835   \n",
      "\n",
      "   satellite_identifier_id  \n",
      "0                        1  \n",
      "1                        2  \n",
      "2                        3  \n",
      "3                        4  \n",
      "4                        5  \n",
      "âœ… Finished processing base_satellite\n",
      "\n",
      "=== base_satelliteentry ===\n",
      "Columns: ['id', 'norad_cat_id', 'name', 'image', 'names', 'status', 'description', 'decayed', 'dashboard_url', 'countries', 'deployed', 'launched', 'website', 'operator_id', 'norad_follow_id', 'approved', 'citation', 'created', 'created_by_id', 'reviewed', 'reviewer_id', 'satellite_identifier_id', 'launch_id', 'receive_review_update', 'review_message']\n",
      "Row count: 9759\n",
      "   id  norad_cat_id         name                              image  \\\n",
      "0   1          7530      OSCAR 7  satellites/AO-7-Model-300x180.gif   \n",
      "1   2         14781      UOSAT 2             satellites/UoSat-2.jpg   \n",
      "2   3         20442        LUSAT             satellites/LUSAT-1.gif   \n",
      "3   4         22826      ITAMSAT           satellites/itamsat_1.jpg   \n",
      "4   5         23439  RADIO ROSTO      satellites/radio-rosto__1.jpg   \n",
      "\n",
      "              names status                                        description  \\\n",
      "0              AO-7  alive  This satellite was a small communications sate...   \n",
      "1  UO-11\\r OSCAR-11  alive  Also known as OSCAR 11, this British built sat...   \n",
      "2             LO-19  alive  Lusat was launched by the same Ariane vehicle ...   \n",
      "3             IO-26  alive  Italy's first amateur radio satellite that use...   \n",
      "4             RS-15  alive  Also known as RS 15. Built by a group of radio...   \n",
      "\n",
      "  decayed dashboard_url countries  ... approved  \\\n",
      "0    None          None            ...        1   \n",
      "1    None          None            ...        1   \n",
      "2    None          None            ...        1   \n",
      "3    None          None            ...        1   \n",
      "4    None          None            ...        1   \n",
      "\n",
      "                                  citation                    created  \\\n",
      "0  CITATION NEEDED - https://xkcd.com/285/ 2021-07-21 10:11:41.973567   \n",
      "1  CITATION NEEDED - https://xkcd.com/285/ 2021-07-21 10:11:41.977018   \n",
      "2  CITATION NEEDED - https://xkcd.com/285/ 2021-07-21 10:11:41.982120   \n",
      "3  CITATION NEEDED - https://xkcd.com/285/ 2021-07-21 10:11:41.985554   \n",
      "4  CITATION NEEDED - https://xkcd.com/285/ 2021-07-21 10:11:41.987219   \n",
      "\n",
      "  created_by_id                   reviewed  reviewer_id  \\\n",
      "0          None 2021-07-21 10:11:41.973567         None   \n",
      "1          None 2021-07-21 10:11:41.977018         None   \n",
      "2          None 2021-07-21 10:11:41.982120         None   \n",
      "3          None 2021-07-21 10:11:41.985554         None   \n",
      "4          None 2021-07-21 10:11:41.987219         None   \n",
      "\n",
      "  satellite_identifier_id launch_id receive_review_update review_message  \n",
      "0                      16      None                     0           None  \n",
      "1                      18      None                     0           None  \n",
      "2                      21      None                     0           None  \n",
      "3                      23      None                     0           None  \n",
      "4                      24      None                     0           None  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "âœ… Finished processing base_satelliteentry\n",
      "\n",
      "=== base_satelliteidentifier ===\n",
      "Columns: ['id', 'sat_id', 'created']\n",
      "Row count: 2920\n",
      "   id                    sat_id                    created\n",
      "0   1  SCHX-0895-2361-9925-0309 2021-07-21 10:11:41.945257\n",
      "1   2  AMOM-6643-5608-9156-4431 2021-07-21 10:11:41.948554\n",
      "2   3  KEFJ-8497-6394-9368-1937 2021-07-21 10:11:41.950438\n",
      "3   4  FBFQ-2056-7966-4855-0749 2021-07-21 10:11:41.952251\n",
      "4   5  BIRW-7828-0822-0647-1194 2021-07-21 10:11:41.954037\n",
      "âœ… Finished processing base_satelliteidentifier\n",
      "\n",
      "=== base_station ===\n",
      "Columns: ['id', 'name', 'image', 'alt', 'lat', 'lng', 'featured_date', 'owner_id', 'created', 'qthlocator', 'last_seen', 'horizon', 'description', 'status', 'testing', 'client_version', 'target_utilization', 'violator_scheduling', 'client_id', 'active_configuration_changed']\n",
      "Row count: 3912\n",
      "   id              name                                              image  \\\n",
      "0   1  Hackerspace.gr 1  ground_stations/269750681_642106180166521_5486...   \n",
      "1   2            KB9JHU                 ground_stations/kb9jhu_P52k3jV.png   \n",
      "2   4            SV1IYO                                                      \n",
      "3   5            oe6xug                         ground_stations/oe6xug.jpg   \n",
      "4   6          Apomahon                                                      \n",
      "\n",
      "   alt        lat        lng featured_date  owner_id             created  \\\n",
      "0  104  38.016970  23.731400    2015-10-11       848 2015-07-22 13:26:49   \n",
      "1  280  39.236000 -86.305000    2017-07-11       165 2015-07-22 14:24:10   \n",
      "2  150  38.024000  23.733000          None       168 2015-10-11 13:59:38   \n",
      "3  330  47.058979  15.460038          None       170 2015-11-23 12:12:51   \n",
      "4  104  38.048000  23.739000    2016-04-25       172 2016-01-17 19:28:26   \n",
      "\n",
      "  qthlocator           last_seen  horizon  \\\n",
      "0     KM18ua 2022-10-05 12:49:26       40   \n",
      "1     EM69uf 2025-07-29 22:39:46        5   \n",
      "2     KM18ua 2024-12-24 09:37:11        0   \n",
      "3     JN77rb 2025-03-24 10:09:13        0   \n",
      "4     KM18ub 2025-11-10 09:53:03       20   \n",
      "\n",
      "                                         description  status  testing  \\\n",
      "0  Yaesu 5500, usrp b200, 2x X-Quad  Antenna 432 ...       0        1   \n",
      "1  Yaesu G-5500 with M2 cross yagi antennas and S...       0        0   \n",
      "2                                                          0        1   \n",
      "3  2025-02-11: back in business triggered by fram...       0        1   \n",
      "4                         Patch 435 MHz , RTL-SDR V3       2        0   \n",
      "\n",
      "           client_version  target_utilization  violator_scheduling client_id  \\\n",
      "0                     1.6                 100                    1             \n",
      "1                   1.8.1                 100                    0             \n",
      "2  1.9.2+0.g4da08be.dirty                 100                    1             \n",
      "3                     1.0                   0                    0             \n",
      "4                   1.8.1                 100                    0             \n",
      "\n",
      "  active_configuration_changed  \n",
      "0                         None  \n",
      "1                         None  \n",
      "2                         None  \n",
      "3                         None  \n",
      "4                         None  \n",
      "âœ… Finished processing base_station\n",
      "\n",
      "=== base_stationstatuslog ===\n",
      "Columns: ['id', 'status', 'changed', 'station_id']\n",
      "Row count: 298893\n",
      "   id  status             changed  station_id\n",
      "0   1       2 2018-04-02 13:55:58           6\n",
      "1   3       2 2018-04-02 13:55:58          12\n",
      "2   4       2 2018-04-02 13:55:58          13\n",
      "3   5       2 2018-04-02 13:55:58          15\n",
      "4   6       2 2018-04-02 13:55:58          16\n",
      "âœ… Finished processing base_stationstatuslog\n",
      "\n",
      "=== base_stationtype ===\n",
      "Columns: ['id', 'name']\n",
      "Row count: 1\n",
      "   id name\n",
      "0   1   RF\n",
      "âœ… Finished processing base_stationtype\n",
      "\n",
      "=== base_telemetry ===\n",
      "Columns: ['id', 'name', 'decoder', 'satellite_id']\n",
      "Row count: 185\n",
      "   id                name decoder  satellite_id\n",
      "0   1           ISS AX.25     iss            28\n",
      "1   2  STRAND-1 Telemetry  strand            87\n",
      "2   3  UNISAT-6 Telemetry     us6           132\n",
      "3   4    FOX-1A Telemetry     fox           178\n",
      "4   5      QBEE Telemetry    qbee           227\n",
      "âœ… Finished processing base_telemetry\n",
      "\n",
      "=== base_transmitterentry ===\n",
      "Columns: ['id', 'uuid', 'description', 'uplink_low', 'uplink_high', 'downlink_low', 'downlink_high', 'invert', 'baud', 'approved', 'downlink_mode_id', 'downlink_drift', 'type', 'uplink_drift', 'citation', 'created', 'status', 'created_by_id', 'service', 'uplink_mode_id', 'reviewed', 'reviewer_id', 'satellite_id', 'iaru_coordination', 'iaru_coordination_url', 'itu_notification', 'unconfirmed', 'receive_review_update', 'review_message']\n",
      "Row count: 9869\n",
      "   id                    uuid                description   uplink_low  \\\n",
      "0   1  ZAKErADdWKpMiDjvKKhmmB                 Mode U TLM          NaN   \n",
      "1   3  ybJ86zjXzQxDReZ5skY56B                 Mode H TLM          NaN   \n",
      "2   5  Zqa2ebzyRRBffvwkLnjTVc           Mode U CW Beacon          NaN   \n",
      "3   6  c4T33yxNiE8EAEc7V6LMQk  Mode V/U APRS,BBS 9K6 FSK  145930000.0   \n",
      "4   7  maYGaaMWsSBeDDDMpcM9ES      Mode V/U BBS1 9K6 FSK  145850000.0   \n",
      "\n",
      "  uplink_high  downlink_low downlink_high  invert    baud  approved  ...  \\\n",
      "0        None     437125000          None       0    12.0         1  ...   \n",
      "1        None      29352000          None       0     0.0         1  ...   \n",
      "2        None     435795000          None       0     0.0         1  ...   \n",
      "3        None     435225000          None       1  9600.0         1  ...   \n",
      "4        None     435225000          None       1  9600.0         1  ...   \n",
      "\n",
      "   uplink_mode_id                   reviewed reviewer_id satellite_id  \\\n",
      "0             NaN 2019-04-18 05:39:53.343316        None           21   \n",
      "1             NaN 2019-04-18 05:39:53.343316        None           24   \n",
      "2             NaN 2019-04-18 05:39:53.343316        None           25   \n",
      "3            72.0 2019-04-18 05:39:53.343316        None           27   \n",
      "4            72.0 2019-04-18 05:39:53.343316        None           27   \n",
      "\n",
      "  iaru_coordination iaru_coordination_url itu_notification unconfirmed  \\\n",
      "0               N/A                           {\"urls\": []}           0   \n",
      "1               N/A                           {\"urls\": []}           0   \n",
      "2               N/A                           {\"urls\": []}           0   \n",
      "3               N/A                           {\"urls\": []}           0   \n",
      "4               N/A                           {\"urls\": []}           0   \n",
      "\n",
      "  receive_review_update  review_message  \n",
      "0                     0            None  \n",
      "1                     0            None  \n",
      "2                     0            None  \n",
      "3                     0            None  \n",
      "4                     0            None  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "âœ… Finished processing base_transmitterentry\n"
     ]
    }
   ],
   "source": [
    "#  --- Explore all relevant tables ---\n",
    "tables_to_explore = [\n",
    "    \"base_antenna\",\n",
    "    \"base_antennatype\",\n",
    "    \"base_frequencyrange\",\n",
    "    \"base_launch\",\n",
    "    \"base_mode\",\n",
    "    \"base_observation\",\n",
    "    \"base_operator\",\n",
    "    \"base_satellite\",\n",
    "    \"base_satelliteentry\",\n",
    "    \"base_satelliteidentifier\",\n",
    "    \"base_station\",\n",
    "    \"base_stationstatuslog\",\n",
    "    \"base_stationtype\",\n",
    "    \"base_telemetry\",\n",
    "    \"base_transmitterentry\"\n",
    "]\n",
    "\n",
    "schema_summaries = []\n",
    "for tbl in tables_to_explore:\n",
    "    summary = explore_table(tbl)\n",
    "    schema_summaries.append(summary)\n",
    "    print(f\"\\n=== {tbl} ===\")\n",
    "    print(\"Columns:\", summary[\"columns\"])\n",
    "    print(\"Row count:\", summary[\"row_count\"])\n",
    "    print(summary[\"preview\"].head())\n",
    "    print(f\"âœ… Finished processing {tbl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7589821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NULL Analysis for: base_observation\n",
      "============================================================\n",
      "Total rows: 12,546,241\n",
      "Sample size for analysis: 10,000\n",
      "\n",
      "Columns with NULL values (20 out of 48):\n",
      "  â€¢ station_antennas                         - 100.00% NULL (10,000 non-NULL)\n",
      "  â€¢ station_lat                              - 100.00% NULL (10,000 non-NULL)\n",
      "  â€¢ station_alt                              - 100.00% NULL (10,000 non-NULL)\n",
      "  â€¢ station_lng                              - 100.00% NULL (10,000 non-NULL)\n",
      "  â€¢ center_frequency                         - 100.00% NULL (9,165 non-NULL)\n",
      "  â€¢ transmitter_status                       - 100.00% NULL (8,757 non-NULL)\n",
      "  â€¢ transmitter_unconfirmed                  - 100.00% NULL (8,757 non-NULL)\n",
      "  â€¢ transmitter_uplink_drift                 - 100.00% NULL (8,757 non-NULL)\n",
      "  â€¢ transmitter_downlink_drift               -  99.98% NULL (10,000 non-NULL)\n",
      "  â€¢ transmitter_uplink_high                  -  99.91% NULL (10,000 non-NULL)\n",
      "  â€¢ transmitter_downlink_high                -  99.91% NULL (10,000 non-NULL)\n",
      "  â€¢ transmitter_uplink_low                   -  94.67% NULL (10,000 non-NULL)\n",
      "  â€¢ transmitter_baud                         -  33.41% NULL (9,953 non-NULL)\n",
      "  â€¢ rise_azimuth                             -  12.43% NULL (10,000 non-NULL)\n",
      "  â€¢ max_altitude                             -  12.43% NULL (10,000 non-NULL)\n",
      "  ... and 5 more columns with NULLs\n",
      "\n",
      "============================================================\n",
      "NULL Analysis for: base_station\n",
      "============================================================\n",
      "Total rows: 3,912\n",
      "Sample size for analysis: 3,912\n",
      "\n",
      "Columns with NULL values (5 out of 20):\n",
      "  â€¢ featured_date                            -  99.90% NULL (3,912 non-NULL)\n",
      "  â€¢ active_configuration_changed             -  79.42% NULL (3,912 non-NULL)\n",
      "  â€¢ last_seen                                -  30.42% NULL (3,912 non-NULL)\n",
      "  â€¢ target_utilization                       -   4.88% NULL (3,912 non-NULL)\n",
      "  â€¢ owner_id                                 -   0.03% NULL (3,912 non-NULL)\n",
      "\n",
      "============================================================\n",
      "NULL Analysis for: base_satelliteentry\n",
      "============================================================\n",
      "Total rows: 9,759\n",
      "Sample size for analysis: 9,759\n",
      "\n",
      "Columns with NULL values (12 out of 25):\n",
      "  â€¢ launch_id                                - 100.00% NULL (9,759 non-NULL)\n",
      "  â€¢ review_message                           -  99.78% NULL (9,406 non-NULL)\n",
      "  â€¢ operator_id                              -  97.24% NULL (9,759 non-NULL)\n",
      "  â€¢ dashboard_url                            -  95.62% NULL (9,759 non-NULL)\n",
      "  â€¢ decayed                                  -  89.79% NULL (9,759 non-NULL)\n",
      "  â€¢ norad_follow_id                          -  80.10% NULL (9,759 non-NULL)\n",
      "  â€¢ deployed                                 -  70.85% NULL (9,759 non-NULL)\n",
      "  â€¢ launched                                 -  28.24% NULL (996 non-NULL)\n",
      "  â€¢ created_by_id                            -  10.57% NULL (427 non-NULL)\n",
      "  â€¢ reviewer_id                              -   9.01% NULL (9,759 non-NULL)\n",
      "  â€¢ norad_cat_id                             -   3.62% NULL (2,845 non-NULL)\n",
      "  â€¢ reviewed                                 -   2.65% NULL (7,003 non-NULL)\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced NULL analysis for all tables ---\n",
    "def detailed_null_analysis(table_name, sample_size=10000):\n",
    "    \"\"\"Perform comprehensive NULL analysis on a table\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"NULL Analysis for: {table_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Get total row count\n",
    "    total_count = pd.read_sql(f\"SELECT COUNT(*) as total FROM {table_name}\", engine)['total'][0]\n",
    "    print(f\"Total rows: {total_count:,}\")\n",
    "    \n",
    "    # Sample data for analysis (if table is large)\n",
    "    df_sample = pd.read_sql(f\"SELECT * FROM {table_name} LIMIT {sample_size}\", engine)\n",
    "    print(f\"Sample size for analysis: {len(df_sample):,}\")\n",
    "    \n",
    "    # Calculate NULL percentages\n",
    "    null_stats = (df_sample.isnull().sum() / len(df_sample) * 100).sort_values(ascending=False)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    null_df = pd.DataFrame({\n",
    "        'column': null_stats.index,\n",
    "        'null_percentage': null_stats.values,\n",
    "        'non_null_count': (len(df_sample) - df_sample.isnull().sum()).values\n",
    "    })\n",
    "    \n",
    "    # Display columns with NULLs\n",
    "    columns_with_nulls = null_df[null_df['null_percentage'] > 0]\n",
    "    \n",
    "    print(f\"\\nColumns with NULL values ({len(columns_with_nulls)} out of {len(df_sample.columns)}):\")\n",
    "    for _, row in columns_with_nulls.head(15).iterrows():\n",
    "        print(f\"  â€¢ {row['column']:40} - {row['null_percentage']:6.2f}% NULL ({row['non_null_count']:,} non-NULL)\")\n",
    "    \n",
    "    if len(columns_with_nulls) > 15:\n",
    "        print(f\"  ... and {len(columns_with_nulls) - 15} more columns with NULLs\")\n",
    "    \n",
    "    return null_df\n",
    "\n",
    "# Analyze key tables\n",
    "key_tables = ['base_observation', 'base_station', 'base_satelliteentry']\n",
    "for table in key_tables:\n",
    "    detailed_null_analysis(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "165ac3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DEEP DIVE: OBSERVATION STATUS ANALYSIS\n",
      "============================================================\n",
      "Top 15 Status Combinations:\n",
      " status  waterfall_status   count  percentage\n",
      "    100               1.0 4613718       36.77\n",
      "   -100               0.0 2624418       20.92\n",
      "      0               NaN 2536168       20.21\n",
      "    100               NaN 1770869       14.11\n",
      "  -1000               NaN  946432        7.54\n",
      "  -1000               0.0   53708        0.43\n",
      "   -100               NaN     731        0.01\n",
      "    100               0.0     142        0.00\n",
      "   -100               1.0      55        0.00\n",
      "\n",
      "----------------------------------------\n",
      "STATUS CODE INTERPRETATION (Hypothesis):\n",
      "----------------------------------------\n",
      "status = 100:    Observation in progress?\n",
      "status = 0:      Successful observation?\n",
      "status = -100:   Failed observation?\n",
      "status = -1000:  Severely failed observation?\n",
      "\n",
      "waterfall_status = 1.0: Waterfall available?\n",
      "waterfall_status = 0.0: No waterfall?\n",
      "waterfall_status = NULL: Unknown/not processed?\n",
      "\n",
      "----------------------------------------\n",
      "LOOKING FOR STATUS DOCUMENTATION IN DATA:\n",
      "----------------------------------------\n",
      "Status combinations with vetted_status:\n",
      " status  waterfall_status vetted_status   count\n",
      "  -1000               NaN           bad      40\n",
      "  -1000               NaN        failed   89740\n",
      "  -1000               NaN          good       4\n",
      "  -1000               NaN       unknown  856648\n",
      "  -1000               0.0        failed   38063\n",
      "  -1000               0.0       unknown   15645\n",
      "   -100               NaN           bad     730\n",
      "   -100               NaN       unknown       1\n",
      "   -100               0.0           bad  564693\n",
      "   -100               0.0        failed      55\n",
      "   -100               0.0          good     575\n",
      "   -100               0.0       unknown 2059095\n",
      "   -100               1.0       unknown      55\n",
      "      0               NaN       unknown 2536168\n",
      "    100               NaN          good    2758\n",
      "    100               NaN       unknown 1768111\n",
      "    100               0.0       unknown     142\n",
      "    100               1.0           bad      53\n",
      "    100               1.0        failed      25\n",
      "    100               1.0          good 1851711\n"
     ]
    }
   ],
   "source": [
    "# --- Deep dive into observation status ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEEP DIVE: OBSERVATION STATUS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get complete status distribution\n",
    "complete_status = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        status,\n",
    "        waterfall_status,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM base_observation \n",
    "    GROUP BY status, waterfall_status\n",
    "    ORDER BY count DESC\n",
    "    LIMIT 15\n",
    "\"\"\", engine)\n",
    "\n",
    "print(\"Top 15 Status Combinations:\")\n",
    "print(complete_status.to_string(index=False))\n",
    "\n",
    "# Interpret the status codes (based on your findings)\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"STATUS CODE INTERPRETATION (Hypothesis):\")\n",
    "print(\"-\"*40)\n",
    "print(\"status = 100:    Observation in progress?\")\n",
    "print(\"status = 0:      Successful observation?\")\n",
    "print(\"status = -100:   Failed observation?\")\n",
    "print(\"status = -1000:  Severely failed observation?\")\n",
    "print(\"\\nwaterfall_status = 1.0: Waterfall available?\")\n",
    "print(\"waterfall_status = 0.0: No waterfall?\")\n",
    "print(\"waterfall_status = NULL: Unknown/not processed?\")\n",
    "\n",
    "# Check if there's any documentation in the data\n",
    "print(\"\\n\" + \"-\"*40)\n",
    "print(\"LOOKING FOR STATUS DOCUMENTATION IN DATA:\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "# Check if status values appear in other columns\n",
    "status_metadata = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        status,\n",
    "        waterfall_status,\n",
    "        vetted_status,\n",
    "        COUNT(*) as count\n",
    "    FROM base_observation \n",
    "    WHERE status IN (0, 100, -100, -1000)\n",
    "    GROUP BY status, waterfall_status, vetted_status\n",
    "    ORDER BY status, waterfall_status\n",
    "    LIMIT 20\n",
    "\"\"\", engine)\n",
    "\n",
    "print(\"Status combinations with vetted_status:\")\n",
    "print(status_metadata.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8499d24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "IDENTIFYING TABLE RELATIONSHIPS\n",
      "============================================================\n",
      "\n",
      "base_observation:\n",
      "  âœ“ ground_station_id -> base_station.id\n",
      "  âœ“ sat_id -> base_satelliteidentifier.sat_id\n",
      "  âœ“ author_id -> base_station.owner_id? (needs verification)\n",
      "\n",
      "base_station:\n",
      "  âœ“ owner_id -> Unknown (possibly user table not in database)\n",
      "\n",
      "base_satellite:\n",
      "  âœ“ satellite_entry_id -> base_satelliteentry.id\n",
      "  âœ“ satellite_identifier_id -> base_satelliteidentifier.id\n",
      "\n",
      "base_antenna:\n",
      "  âœ“ station_id -> base_station.id\n",
      "  âœ“ antenna_type_id -> base_antennatype.id\n",
      "\n",
      "base_frequencyrange:\n",
      "  âœ“ antenna_id -> base_antenna.id\n"
     ]
    }
   ],
   "source": [
    "# --- Identify foreign key relationships ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"IDENTIFYING TABLE RELATIONSHIPS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Based on column names, identify potential foreign keys\n",
    "table_relationships = {\n",
    "    'base_observation': {\n",
    "        'ground_station_id': 'base_station.id',\n",
    "        'sat_id': 'base_satelliteidentifier.sat_id',\n",
    "        'author_id': 'base_station.owner_id? (needs verification)'\n",
    "    },\n",
    "    'base_station': {\n",
    "        'owner_id': 'Unknown (possibly user table not in database)'\n",
    "    },\n",
    "    'base_satellite': {\n",
    "        'satellite_entry_id': 'base_satelliteentry.id',\n",
    "        'satellite_identifier_id': 'base_satelliteidentifier.id'\n",
    "    },\n",
    "    'base_antenna': {\n",
    "        'station_id': 'base_station.id',\n",
    "        'antenna_type_id': 'base_antennatype.id'\n",
    "    },\n",
    "    'base_frequencyrange': {\n",
    "        'antenna_id': 'base_antenna.id'\n",
    "    }\n",
    "}\n",
    "\n",
    "for table, relationships in table_relationships.items():\n",
    "    print(f\"\\n{table}:\")\n",
    "    for fk_col, ref_table in relationships.items():\n",
    "        # Check if column exists in table\n",
    "        try:\n",
    "            df_check = pd.read_sql(f\"SELECT {fk_col} FROM {table} LIMIT 1\", engine)\n",
    "            print(f\"  âœ“ {fk_col} -> {ref_table}\")\n",
    "        except:\n",
    "            print(f\"  âœ— {fk_col} (column might not exist)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "186e6c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "NUMERICAL COLUMNS SUMMARY IN OBSERVATIONS\n",
      "============================================================\n",
      "Summary of numerical columns (sample):\n",
      "\n",
      "max_altitude:\n",
      "Could not compute numerical summaries: unsupported format string passed to Series.__format__\n",
      "max_altitude: 12,544,939 non-NULL rows\n",
      "rise_azimuth: 12,544,939 non-NULL rows\n",
      "set_azimuth: 12,544,939 non-NULL rows\n"
     ]
    }
   ],
   "source": [
    "# --- Summary statistics for numerical columns in observations ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NUMERICAL COLUMNS SUMMARY IN OBSERVATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get summary for columns that might have data\n",
    "numerical_cols = ['max_altitude', 'rise_azimuth', 'set_azimuth', 'station_alt', 'station_lat', 'station_lng']\n",
    "\n",
    "summary_queries = []\n",
    "for col in numerical_cols:\n",
    "    summary_queries.append(f\"\"\"\n",
    "        '{col}' as column_name,\n",
    "        COUNT({col}) as non_null_count,\n",
    "        ROUND(COUNT({col}) * 100.0 / COUNT(*), 2) as non_null_percentage,\n",
    "        ROUND(AVG({col}), 4) as avg_value,\n",
    "        ROUND(MIN({col}), 4) as min_value,\n",
    "        ROUND(MAX({col}), 4) as max_value,\n",
    "        ROUND(STD({col}), 4) as std_dev\n",
    "    \"\"\")\n",
    "\n",
    "summary_query = f\"\"\"\n",
    "SELECT {', '.join(summary_queries)}\n",
    "FROM base_observation\n",
    "WHERE 1=1\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    numerical_summary = pd.read_sql(summary_query, engine)\n",
    "    print(\"Summary of numerical columns (sample):\")\n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        row = numerical_summary.iloc[i]\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Non-NULL: {row['non_null_percentage']}% ({row['non_null_count']:,} rows)\")\n",
    "        if row['non_null_percentage'] > 0:\n",
    "            print(f\"  Avg: {row['avg_value']}, Min: {row['min_value']}, Max: {row['max_value']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute numerical summaries: {e}\")\n",
    "    # Fallback to simpler query\n",
    "    for col in numerical_cols[:3]:  # Just check first few\n",
    "        simple_check = pd.read_sql(f\"\"\"\n",
    "            SELECT COUNT({col}) as non_null_count \n",
    "            FROM base_observation \n",
    "            WHERE {col} IS NOT NULL\n",
    "        \"\"\", engine)\n",
    "        print(f\"{col}: {simple_check['non_null_count'][0]:,} non-NULL rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e37b3760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY RECOMMENDATIONS FOR ML PIPELINE\n",
      "============================================================\n",
      "\n",
      "[CRITICAL] Define target variable\n",
      "   Need to map status codes (0, 100, -100, -1000) to success/failure labels\n",
      "\n",
      "[HIGH] Handle missing station data\n",
      "   station_lat, station_lng, station_alt are 100% NULL in observation table.      Need to join with base_station table\n",
      "\n",
      "[HIGH] Investigate status code meanings\n",
      "   Status codes need interpretation. Check: status=0 with waterfall_status=NULL might be successful\n",
      "\n",
      "[MEDIUM] Calculate derived features\n",
      "   Compute: duration = end - start, time_of_day from start, day_of_week\n",
      "\n",
      "[MEDIUM] Clean waterfall_status\n",
      "   Has both numeric (0.0, 1.0) and NULL values. Convert to categorical\n",
      "\n",
      "[LOW] Investigate vetted_status\n",
      "   Column exists but not explored. Might provide ground truth labels\n"
     ]
    }
   ],
   "source": [
    "# --- Data Quality Recommendations ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY RECOMMENDATIONS FOR ML PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "recommendations = [\n",
    "    (\"CRITICAL\", \"Define target variable\", \n",
    "     \"Need to map status codes (0, 100, -100, -1000) to success/failure labels\"),\n",
    "    \n",
    "    (\"HIGH\", \"Handle missing station data\", \n",
    "     \"station_lat, station_lng, station_alt are 100% NULL in observation table. \\\n",
    "     Need to join with base_station table\"),\n",
    "    \n",
    "    (\"HIGH\", \"Investigate status code meanings\", \n",
    "     \"Status codes need interpretation. Check: status=0 with waterfall_status=NULL might be successful\"),\n",
    "    \n",
    "    (\"MEDIUM\", \"Calculate derived features\", \n",
    "     \"Compute: duration = end - start, time_of_day from start, day_of_week\"),\n",
    "    \n",
    "    (\"MEDIUM\", \"Clean waterfall_status\", \n",
    "     \"Has both numeric (0.0, 1.0) and NULL values. Convert to categorical\"),\n",
    "    \n",
    "    (\"LOW\", \"Investigate vetted_status\", \n",
    "     \"Column exists but not explored. Might provide ground truth labels\")\n",
    "]\n",
    "\n",
    "for priority, item, details in recommendations:\n",
    "    print(f\"\\n[{priority}] {item}\")\n",
    "    print(f\"   {details}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19acb6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Enhanced schema documentation saved to 'enhanced_schema_document.json'\n"
     ]
    }
   ],
   "source": [
    "# --- Save enhanced schema documentation ---\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Create comprehensive schema document\n",
    "schema_document = {\n",
    "    \"analysis_date\": datetime.now().isoformat(),\n",
    "    \"database\": DB_NAME,\n",
    "    \"tables\": {},\n",
    "    \"key_findings\": {\n",
    "        \"observation_status_codes\": {\n",
    "            \"100\": \"Most common (46.1M rows) - possibly 'in progress'\",\n",
    "            \"-100\": \"2nd most common (26.2M rows) - possibly 'failed'\",\n",
    "            \"0\": \"2.5M rows - possibly 'successful'\",\n",
    "            \"-1000\": \"1M rows - possibly 'severely failed'\"\n",
    "        },\n",
    "        \"data_quality_issues\": [\n",
    "            \"station_* columns 100% NULL in observations (need join)\",\n",
    "            \"Many transmitter_* columns mostly NULL\",\n",
    "            \"Need to interpret status codes\"\n",
    "        ],\n",
    "        \"recommended_target_variable\": \"Map status=0 as success, others as failure (to be validated)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add table details\n",
    "for summary in schema_summaries:\n",
    "    table_name = summary[\"table\"]\n",
    "    schema_document[\"tables\"][table_name] = {\n",
    "        \"row_count\": int(summary[\"row_count\"]),\n",
    "        \"columns\": summary[\"columns\"],\n",
    "        \"preview_sample\": summary[\"preview\"].head(3).to_dict(orient='records')\n",
    "    }\n",
    "\n",
    "# Save as JSON\n",
    "with open(\"enhanced_schema_document.json\", \"w\") as f:\n",
    "    json.dump(schema_document, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nðŸ“Š Enhanced schema documentation saved to 'enhanced_schema_document.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07ae935a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Saved preview for base_antenna\n",
      "ðŸ“„ Saved preview for base_antennatype\n",
      "ðŸ“„ Saved preview for base_frequencyrange\n",
      "ðŸ“„ Saved preview for base_launch\n",
      "ðŸ“„ Saved preview for base_mode\n",
      "ðŸ“„ Saved preview for base_observation\n",
      "ðŸ“„ Saved preview for base_operator\n",
      "ðŸ“„ Saved preview for base_satellite\n",
      "ðŸ“„ Saved preview for base_satelliteentry\n",
      "ðŸ“„ Saved preview for base_satelliteidentifier\n",
      "ðŸ“„ Saved preview for base_station\n",
      "ðŸ“„ Saved preview for base_stationstatuslog\n",
      "ðŸ“„ Saved preview for base_stationtype\n",
      "ðŸ“„ Saved preview for base_telemetry\n",
      "ðŸ“„ Saved preview for base_transmitterentry\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save to Markdown with fallback\n",
    "import importlib \n",
    "with open(\"schema_summary.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"# ðŸ“Š SatNOGS Dataset Schema Summary\\n\\n\")\n",
    "    for s in schema_summaries:\n",
    "        f.write(f\"## Table: {s['table']}\\n\")\n",
    "        f.write(f\"- Row count: {s['row_count']}\\n\")\n",
    "        f.write(f\"- Columns: {', '.join(s['columns'])}\\n\\n\")\n",
    "        f.write(\"### Preview (first 5 rows)\\n\")\n",
    "        \n",
    "        # Try to use to_markdown if tabulate is available\n",
    "        try:\n",
    "            importlib.import_module(\"tabulate\")\n",
    "            f.write(s[\"preview\"].head().to_markdown(index=False))\n",
    "        except ImportError:\n",
    "            # Fallback to plain text if tabulate is missing\n",
    "            f.write(s[\"preview\"].head().to_string(index=False))\n",
    "        \n",
    "        f.write(\"\\n\\n---\\n\\n\")\n",
    "        print(f\"ðŸ“„ Saved preview for {s['table']}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
