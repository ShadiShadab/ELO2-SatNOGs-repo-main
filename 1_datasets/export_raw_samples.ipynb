{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f6c006d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì° SatNOGS Data Export Script\n",
      "Execution started: 2025-12-11 14:48:32\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# 1_datasets/export_raw_samples.py\n",
    "\"\"\"\n",
    "Export sample data from SatNOGS database for capstone project.\n",
    "Samples large tables and exports full small tables.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "print(f\"üì° SatNOGS Data Export Script\")\n",
    "print(f\"Execution started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Database connection\n",
    "DB_USER = \"root\"\n",
    "DB_PASSWORD = \"123456789\"\n",
    "DB_HOST = \"127.0.0.1\"\n",
    "DB_PORT = \"3306\"\n",
    "DB_NAME = \"satnogs\"\n",
    "\n",
    "engine = create_engine(f\"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\")\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(\"1_datasets/raw\", exist_ok=True)\n",
    "os.makedirs(\"1_datasets/metadata\", exist_ok=True)\n",
    "\n",
    "def run_query(sql, desc=\"Query\"):\n",
    "    \"\"\"Run SQL query with timing.\"\"\"\n",
    "    print(f\"\\nüîç {desc}...\")\n",
    "    start = time.time()\n",
    "    df = pd.read_sql_query(sql, engine)\n",
    "    print(f\"‚úÖ Retrieved {len(df):,} rows in {time.time()-start:.2f}s\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a015cbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 1: EXPORTING METADATA AND TIMELINE\n",
      "======================================================================\n",
      "\n",
      "üîç Dataset timeline overview...\n",
      "‚úÖ Retrieved 1 rows in 77.91s\n",
      "üìä Timeline saved to: 1_datasets/metadata/dataset_timeline.csv\n",
      "\n",
      "üîç Table statistics...\n",
      "‚úÖ Retrieved 0 rows in 0.01s\n",
      "üìà Table statistics saved\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. Export TIMELINE and METADATA first\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 1: EXPORTING METADATA AND TIMELINE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Dataset timeline\n",
    "timeline = run_query(\"\"\"\n",
    "    SELECT \n",
    "        MIN(start) AS first_observation,\n",
    "        MAX(start) AS last_observation,\n",
    "        COUNT(*) AS total_observations,\n",
    "        COUNT(DISTINCT ground_station_id) AS unique_stations,\n",
    "        COUNT(DISTINCT sat_id) AS unique_satellites\n",
    "    FROM base_observation\n",
    "    WHERE start IS NOT NULL\n",
    "\"\"\", \"Dataset timeline overview\")\n",
    "\n",
    "timeline.to_csv(\"1_datasets/metadata/dataset_timeline.csv\", index=False)\n",
    "print(\"üìä Timeline saved to: 1_datasets/metadata/dataset_timeline.csv\")\n",
    "\n",
    "# Table statistics\n",
    "table_stats = run_query(\"\"\"\n",
    "    SELECT \n",
    "        TABLE_NAME as table_name,\n",
    "        TABLE_ROWS as estimated_rows,\n",
    "        CREATE_TIME as created,\n",
    "        UPDATE_TIME as last_updated\n",
    "    FROM information_schema.TABLES \n",
    "    WHERE TABLE_SCHEMA = 'newdata'\n",
    "    ORDER BY TABLE_ROWS DESC\n",
    "\"\"\", \"Table statistics\")\n",
    "\n",
    "table_stats.to_csv(\"1_datasets/metadata/table_statistics.csv\", index=False)\n",
    "print(\"üìà Table statistics saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631587d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 2: EXPORTING SMALL TABLES (FULL)\n",
      "======================================================================\n",
      "\n",
      "üîç Exporting base_antennatype...\n",
      "‚úÖ Retrieved 17 rows in 0.01s\n",
      "üíæ Saved: 1_datasets/raw/base_antennatype_full.csv\n",
      "\n",
      "üîç Exporting base_stationtype...\n",
      "‚úÖ Retrieved 1 rows in 0.01s\n",
      "üíæ Saved: 1_datasets/raw/base_stationtype_full.csv\n",
      "\n",
      "üîç Exporting base_operator...\n",
      "‚úÖ Retrieved 6 rows in 0.01s\n",
      "üíæ Saved: 1_datasets/raw/base_operator_full.csv\n",
      "\n",
      "üîç Exporting base_mode...\n",
      "‚úÖ Retrieved 56 rows in 0.01s\n",
      "üíæ Saved: 1_datasets/raw/base_mode_full.csv\n",
      "\n",
      "üîç Exporting base_telemetry...\n",
      "‚úÖ Retrieved 185 rows in 0.01s\n",
      "üíæ Saved: 1_datasets/raw/base_telemetry_full.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 3. Export FULL small tables (< 10K rows)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 2: EXPORTING SMALL TABLES (FULL)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "small_tables = [\n",
    "    'base_antennatype',      # 17 rows\n",
    "    'base_stationtype',      # 1 row\n",
    "    'base_operator',         # 6 rows\n",
    "    'base_mode',             # 56 rows\n",
    "    'base_telemetry',        # 185 rows\n",
    "]\n",
    "\n",
    "for table in small_tables:\n",
    "    df = run_query(f\"SELECT * FROM {table}\", f\"Exporting {table}\")\n",
    "    df.to_csv(f\"1_datasets/raw/{table}_full.csv\", index=False)\n",
    "    print(f\"üíæ Saved: 1_datasets/raw/{table}_full.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52a3679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 3: EXPORTING MEDIUM TABLES\n",
      "======================================================================\n",
      "\n",
      "üîç Exporting base_station (full)...\n",
      "‚úÖ Retrieved 3,912 rows in 0.34s\n",
      "üíæ Saved: 1_datasets/raw/base_station_sample.csv\n",
      "\n",
      "üîç Exporting base_satelliteentry (sample: 5000)...\n",
      "‚úÖ Retrieved 5,000 rows in 0.69s\n",
      "üíæ Saved: 1_datasets/raw/base_satelliteentry_sample.csv\n",
      "\n",
      "üîç Exporting base_transmitterentry (sample: 5000)...\n",
      "‚úÖ Retrieved 5,000 rows in 0.41s\n",
      "üíæ Saved: 1_datasets/raw/base_transmitterentry_sample.csv\n",
      "\n",
      "üîç Exporting base_satellite (sample: 3000)...\n",
      "‚úÖ Retrieved 2,903 rows in 0.08s\n",
      "üíæ Saved: 1_datasets/raw/base_satellite_sample.csv\n",
      "\n",
      "üîç Exporting base_satelliteidentifier (sample: 3000)...\n",
      "‚úÖ Retrieved 2,920 rows in 0.07s\n",
      "üíæ Saved: 1_datasets/raw/base_satelliteidentifier_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 4. Export MEDIUM tables with sampling if needed\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 3: EXPORTING MEDIUM TABLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "medium_tables = {\n",
    "    'base_station': 'full',          # 3,912 rows\n",
    "    'base_satelliteentry': 5000,     # Sample 5K from 9,759\n",
    "    'base_transmitterentry': 5000,   # Sample 5K from 9,869\n",
    "    'base_satellite': 3000,          # Sample 3K from 2,903\n",
    "    'base_satelliteidentifier': 3000,# Sample 3K from 2,920\n",
    "}\n",
    "\n",
    "for table, sample_size in medium_tables.items():\n",
    "    if sample_size == 'full':\n",
    "        df = run_query(f\"SELECT * FROM {table}\", f\"Exporting {table} (full)\")\n",
    "    else:\n",
    "        df = run_query(f\"\"\"\n",
    "            SELECT * FROM {table} \n",
    "            ORDER BY RAND() \n",
    "            LIMIT {sample_size}\n",
    "        \"\"\", f\"Exporting {table} (sample: {sample_size})\")\n",
    "    \n",
    "    df.to_csv(f\"1_datasets/raw/{table}_sample.csv\", index=False)\n",
    "    print(f\"üíæ Saved: 1_datasets/raw/{table}_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8173beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 4: EXPORTING LARGE TABLES (STRATEGIC SAMPLING)\n",
      "======================================================================\n",
      "\n",
      "üìä Getting observation sample with balanced status...\n",
      "\n",
      "üîç Status distribution...\n",
      "‚úÖ Retrieved 4 rows in 70.48s\n",
      "\n",
      "Status distribution in full dataset:\n",
      " status   count\n",
      "    100 6384729\n",
      "   -100 2625204\n",
      "      0 2536168\n",
      "  -1000 1000140\n",
      "\n",
      "üîç Sampling 100K observations (balanced by status)...\n",
      "‚úÖ Retrieved 100,000 rows in 396.84s\n",
      "üíæ Saved: 1_datasets/raw/base_observation_sample_100k.csv (100,000 rows)\n",
      "\n",
      "üîç Latest observations (2025)...\n",
      "‚úÖ Retrieved 50,000 rows in 5.33s\n",
      "üíæ Saved: 1_datasets/raw/base_observation_latest_50k.csv (50,000 rows)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. Export LARGE tables with strategic sampling\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 4: EXPORTING LARGE TABLES (STRATEGIC SAMPLING)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Strategy: Get balanced sample of observations by status and year\n",
    "print(\"\\nüìä Getting observation sample with balanced status...\")\n",
    "\n",
    "# First, get status distribution\n",
    "status_dist = run_query(\"\"\"\n",
    "    SELECT status, COUNT(*) as count\n",
    "    FROM base_observation \n",
    "    GROUP BY status \n",
    "    ORDER BY count DESC\n",
    "\"\"\", \"Status distribution\")\n",
    "\n",
    "print(\"\\nStatus distribution in full dataset:\")\n",
    "print(status_dist.to_string(index=False))\n",
    "\n",
    "# Sample observations: 100K total, balanced by status where possible\n",
    "observation_sample = run_query(\"\"\"\n",
    "    WITH status_counts AS (\n",
    "        SELECT status, COUNT(*) as total\n",
    "        FROM base_observation\n",
    "        GROUP BY status\n",
    "    )\n",
    "    SELECT o.*\n",
    "    FROM base_observation o\n",
    "    WHERE (\n",
    "        -- For common statuses, sample proportionally\n",
    "        (o.status IN (100, -100, 0, -1000) AND RAND() < 50000.0 / (SELECT total FROM status_counts WHERE status = o.status))\n",
    "        OR\n",
    "        -- For rare statuses, take all\n",
    "        (o.status NOT IN (100, -100, 0, -1000))\n",
    "    )\n",
    "    ORDER BY o.start\n",
    "    LIMIT 100000\n",
    "\"\"\", \"Sampling 100K observations (balanced by status)\")\n",
    "\n",
    "observation_sample.to_csv(\"1_datasets/raw/base_observation_sample_100k.csv\", index=False)\n",
    "print(f\"üíæ Saved: 1_datasets/raw/base_observation_sample_100k.csv ({len(observation_sample):,} rows)\")\n",
    "\n",
    "# Also get a time-based sample (latest observations)\n",
    "latest_observations = run_query(\"\"\"\n",
    "    SELECT * \n",
    "    FROM base_observation \n",
    "    WHERE start >= '2025-01-01'\n",
    "    ORDER BY start DESC\n",
    "    LIMIT 50000\n",
    "\"\"\", \"Latest observations (2025)\")\n",
    "\n",
    "latest_observations.to_csv(\"1_datasets/raw/base_observation_latest_50k.csv\", index=False)\n",
    "print(f\"üíæ Saved: 1_datasets/raw/base_observation_latest_50k.csv ({len(latest_observations):,} rows)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f11c2569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 5: EXPORTING RELATIONSHIP TABLES\n",
      "======================================================================\n",
      "\n",
      "üîç Exporting base_antenna (sample: 2000)...\n",
      "‚úÖ Retrieved 2,000 rows in 0.08s\n",
      "üíæ Saved: 1_datasets/raw/base_antenna_sample.csv\n",
      "\n",
      "üîç Exporting base_frequencyrange (sample: 2000)...\n",
      "‚úÖ Retrieved 2,000 rows in 0.05s\n",
      "üíæ Saved: 1_datasets/raw/base_frequencyrange_sample.csv\n",
      "\n",
      "üîç Exporting base_stationstatuslog (sample: 10000)...\n",
      "‚úÖ Retrieved 10,000 rows in 0.71s\n",
      "üíæ Saved: 1_datasets/raw/base_stationstatuslog_sample.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. Export RELATIONSHIP tables\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 5: EXPORTING RELATIONSHIP TABLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "relationship_tables = {\n",
    "    'base_antenna': 2000,           # Sample 2K from 4,437\n",
    "    'base_frequencyrange': 2000,    # Sample 2K from 5,311\n",
    "    'base_stationstatuslog': 10000, # Sample 10K from 298,893\n",
    "}\n",
    "\n",
    "for table, sample_size in relationship_tables.items():\n",
    "    df = run_query(f\"\"\"\n",
    "        SELECT * FROM {table} \n",
    "        ORDER BY RAND() \n",
    "        LIMIT {sample_size}\n",
    "    \"\"\", f\"Exporting {table} (sample: {sample_size})\")\n",
    "    \n",
    "    df.to_csv(f\"1_datasets/raw/{table}_sample.csv\", index=False)\n",
    "    print(f\"üíæ Saved: 1_datasets/raw/{table}_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a687902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 6: CREATING DATA DICTIONARY\n",
      "======================================================================\n",
      "üìñ Data dictionary saved to: 1_datasets/metadata/data_dictionary.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 7. Create DATA DICTIONARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 6: CREATING DATA DICTIONARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create basic data dictionary from schema\n",
    "data_dict = []\n",
    "\n",
    "for table in ['base_observation', 'base_station', 'base_satelliteentry', 'base_transmitterentry']:\n",
    "    # Get column info\n",
    "    df_sample = pd.read_sql_query(f\"SELECT * FROM {table} LIMIT 100\", engine)\n",
    "    \n",
    "    for col in df_sample.columns:\n",
    "        data_dict.append({\n",
    "            'table': table,\n",
    "            'column': col,\n",
    "            'dtype': str(df_sample[col].dtype),\n",
    "            'non_null_sample': df_sample[col].notnull().sum(),\n",
    "            'sample_values': str(df_sample[col].dropna().unique()[:3].tolist() if df_sample[col].notnull().sum() > 0 else 'ALL_NULL')\n",
    "        })\n",
    "\n",
    "data_dict_df = pd.DataFrame(data_dict)\n",
    "data_dict_df.to_csv(\"1_datasets/metadata/data_dictionary.csv\", index=False)\n",
    "print(\"üìñ Data dictionary saved to: 1_datasets/metadata/data_dictionary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13e4ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STEP 7: GENERATING SUMMARY REPORT\n",
      "======================================================================\n",
      "üìÑ Summary report saved to: 1_datasets/README.md\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 8. Generate SUMMARY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STEP 7: GENERATING SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate total exported data size\n",
    "total_rows = 0\n",
    "exported_files = []\n",
    "\n",
    "for file in os.listdir(\"1_datasets/raw\"):\n",
    "    if file.endswith('.csv'):\n",
    "        filepath = os.path.join(\"1_datasets/raw\", file)\n",
    "        df_temp = pd.read_csv(filepath, nrows=1)  # Just to check\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            row_count = sum(1 for line in f) - 1  # Subtract header\n",
    "        \n",
    "        exported_files.append({\n",
    "            'file': file,\n",
    "            'rows': row_count,\n",
    "            'size_mb': os.path.getsize(filepath) / (1024*1024)\n",
    "        })\n",
    "        total_rows += row_count\n",
    "\n",
    "summary_df = pd.DataFrame(exported_files)\n",
    "\n",
    "# Create summary markdown\n",
    "summary_md = f\"\"\"# üìä SatNOGS Dataset Exports\n",
    "*Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "\n",
    "## Dataset Overview\n",
    "- **Source**: SatNOGS Database (MySQL/MariaDB)\n",
    "- **Time Range**: {timeline['first_observation'].iloc[0]} to {timeline['last_observation'].iloc[0]}\n",
    "- **Total Observations in DB**: {timeline['total_observations'].iloc[0]:,}\n",
    "- **Unique Stations**: {timeline['unique_stations'].iloc[0]:,}\n",
    "- **Unique Satellites**: {timeline['unique_satellites'].iloc[0]:,}\n",
    "\n",
    "## Exported Files Summary\n",
    "Total exported rows: {total_rows:,}\n",
    "\n",
    "| File | Rows | Size (MB) |\n",
    "|------|------|-----------|\n",
    "\"\"\"\n",
    "\n",
    "for _, row in summary_df.iterrows():\n",
    "    summary_md += f\"| {row['file']} | {row['rows']:,} | {row['size_mb']:.2f} |\\n\"\n",
    "\n",
    "summary_md += f\"\"\"\n",
    "## File Descriptions\n",
    "\n",
    "### Raw Data Files (`1_datasets/raw/`)\n",
    "- `base_observation_sample_100k.csv`: Balanced sample of 100K observations by status\n",
    "- `base_observation_latest_50k.csv`: Latest observations from 2025\n",
    "- `base_station_sample.csv`: All ground stations (full export)\n",
    "- `base_satelliteentry_sample.csv`: Sample of satellite metadata\n",
    "- `base_transmitterentry_sample.csv`: Sample of transmitter configurations\n",
    "- `base_antennatype_full.csv`: Complete antenna type catalog\n",
    "- ... and other relationship tables\n",
    "\n",
    "### Metadata Files (`1_datasets/metadata/`)\n",
    "- `dataset_timeline.csv`: Time range and counts\n",
    "- `table_statistics.csv`: Row counts for all tables\n",
    "- `data_dictionary.csv`: Column descriptions and sample values\n",
    "\n",
    "## Usage Notes\n",
    "1. All CSV files use UTF-8 encoding\n",
    "2. Files are sampled for memory efficiency while maintaining distributions\n",
    "3. For full analysis, use database connection with appropriate sampling\n",
    "4. API endpoints available at:\n",
    "   - https://network.satnogs.org/api/\n",
    "   - https://db.satnogs.org/api/\n",
    "\n",
    "## Known Issues\n",
    "1. Some observation columns have high NULL percentages (see data dictionary)\n",
    "2. Station location data needs joining with `base_station` table\n",
    "3. Status codes need interpretation mapping\n",
    "\"\"\"\n",
    "\n",
    "with open(\"1_datasets/README.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(summary_md)\n",
    "\n",
    "print(\"üìÑ Summary report saved to: 1_datasets/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66167ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "‚úÖ DATA EXPORT COMPLETE\n",
      "======================================================================\n",
      "\n",
      "üìÅ Files saved in:\n",
      "   - 1_datasets/raw/      (data files)\n",
      "   - 1_datasets/metadata/ (documentation)\n",
      "\n",
      "üìä Total exported: 187,566 rows (211.31 MB)\n",
      "\n",
      "üéØ Next steps:\n",
      "   1. Review 1_datasets/README.md\n",
      "   2. Move to 2_data_preparation/ for feature engineering\n",
      "   3. Use sampled data for EDA in 3_data_exploration/\n",
      "\n",
      "‚è±Ô∏è  Total execution time: 0.00 seconds\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPLETION\n",
    "# ============================================================================\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ DATA EXPORT COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìÅ Files saved in:\")\n",
    "print(f\"   - 1_datasets/raw/      (data files)\")\n",
    "print(f\"   - 1_datasets/metadata/ (documentation)\")\n",
    "\n",
    "total_size_mb = sum(f['size_mb'] for f in exported_files)\n",
    "print(f\"\\nüìä Total exported: {total_rows:,} rows ({total_size_mb:.2f} MB)\")\n",
    "\n",
    "print(\"\\nüéØ Next steps:\")\n",
    "print(\"   1. Review 1_datasets/README.md\")\n",
    "print(\"   2. Move to 2_data_preparation/ for feature engineering\")\n",
    "print(\"   3. Use sampled data for EDA in 3_data_exploration/\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Total execution time: {time.time() - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
