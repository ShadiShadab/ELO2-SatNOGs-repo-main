{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3661130f",
   "metadata": {},
   "source": [
    "# 2_data_preparation/02_create_four_year_dataset.ipynb\n",
    "\"\"\"\n",
    "# Create Four-Year Observation Dataset (2021-2025)\n",
    "\n",
    "## Goal\n",
    "Load a representative sample of observations from the last 4 years (2021-2025) \n",
    "and prepare it for ML modeling with proper station data joins.\n",
    "\n",
    "## Fix Applied:\n",
    "1. **Random Sampling**: Use `ORDER BY RAND()` instead of `ORDER BY start DESC` \n",
    "   to get a representative sample from the entire 4-year period\n",
    "2. **Station Join Debugging**: Added diagnostics to understand join issues\n",
    "3. **Data Validation**: Enhanced checks for temporal coverage\n",
    "\n",
    "## Steps in This Notebook:\n",
    "1. **Database Connection**: Connect to MariaDB\n",
    "2. **Data Extraction**: Query RANDOM sample from last 4 years\n",
    "3. **Join Operations**: Merge with station data + diagnostics\n",
    "4. **Target Definition**: Create success/failure labels\n",
    "5. **Feature Engineering**: Create temporal, geometric features\n",
    "6. **Data Cleaning**: Handle missing values\n",
    "7. **Dataset Export**: Save processed dataset\n",
    "\n",
    "## Expected Output:\n",
    "- A cleaned dataset of ~1M observations spanning 2021-2025\n",
    "- Proper station data joins\n",
    "- Seasonal representation (all 4 seasons)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d12149b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸ“¡ CREATING FOUR-YEAR OBSERVATION DATASET (2021-2025)\n",
      "================================================================================\n",
      "Execution started: 2025-12-11 16:44:05\n",
      "Working directory: d:\\ELO 2\\satnogs_project\\satellite-pass-prediction\\2_data_preparation\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORT LIBRARIES\n",
    "# ============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine, text\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ðŸ“¡ CREATING FOUR-YEAR OBSERVATION DATASET (2021-2025)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Execution started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccd4f794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 1: DATABASE CONNECTION\n",
      "================================================================================\n",
      "Connecting to database: satnogs on 127.0.0.1:3306\n",
      "User: root\n",
      "âœ… Database connection successful!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: DATABASE CONNECTION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: DATABASE CONNECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Database credentials\n",
    "DB_USER = \"root\"\n",
    "DB_PASSWORD = \"123456789\"\n",
    "DB_HOST = \"127.0.0.1\"\n",
    "DB_PORT = \"3306\"\n",
    "DB_NAME = \"satnogs\"\n",
    "\n",
    "# Create connection string\n",
    "connection_string = f\"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "\n",
    "print(f\"Connecting to database: {DB_NAME} on {DB_HOST}:{DB_PORT}\")\n",
    "print(f\"User: {DB_USER}\")\n",
    "\n",
    "try:\n",
    "    engine = create_engine(connection_string)\n",
    "    \n",
    "    # Test connection with a simple query\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(\"SELECT 1 as test\"))\n",
    "        test_value = result.fetchone()[0]\n",
    "    \n",
    "    if test_value == 1:\n",
    "        print(\"âœ… Database connection successful!\")\n",
    "    else:\n",
    "        print(\"âŒ Database connection test failed\")\n",
    "        raise ConnectionError(\"Could not connect to database\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error connecting to database: {e}\")\n",
    "    print(\"Please check:\")\n",
    "    print(\"  1. Database is running\")\n",
    "    print(\"  2. Credentials are correct\")\n",
    "    print(\"  3. Port 3306 is open\")\n",
    "    raise SystemExit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dcd46ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 2: DETERMINING DATE RANGE\n",
      "================================================================================\n",
      "Finding most recent observation date...\n",
      "âœ… Latest observation date: 2025-11-12 09:59:41\n",
      "ðŸ“… 4-year cutoff date: 2021-11-12 09:59:41\n",
      "â±ï¸  Time period: 2021-11-12 to 2025-11-12\n",
      "ðŸ“Š Total observations in last 4 years: 7,560,410\n",
      "ðŸ“Š Sampling 1,000,000 rows (13.23% of total)\n",
      "â±ï¸  Date determination completed in 97.79s\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: DETERMINE DATE RANGE FOR LAST 4 YEARS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: DETERMINING DATE RANGE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get the most recent observation date\n",
    "print(\"Finding most recent observation date...\")\n",
    "start_time = time.time()\n",
    "\n",
    "max_date_query = \"\"\"\n",
    "SELECT MAX(start) as latest_date FROM base_observation WHERE start IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    max_date_df = pd.read_sql(max_date_query, engine)\n",
    "    latest_date = pd.to_datetime(max_date_df['latest_date'].iloc[0])\n",
    "    cutoff_date = latest_date - pd.DateOffset(years=4)\n",
    "    \n",
    "    print(f\"âœ… Latest observation date: {latest_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"ðŸ“… 4-year cutoff date: {cutoff_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"â±ï¸  Time period: {cutoff_date.strftime('%Y-%m-%d')} to {latest_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    # Get total count for the 4-year period\n",
    "    count_query = f\"\"\"\n",
    "    SELECT COUNT(*) as total_count \n",
    "    FROM base_observation \n",
    "    WHERE start >= '{cutoff_date.strftime('%Y-%m-%d')}'\n",
    "      AND max_altitude IS NOT NULL\n",
    "      AND status IS NOT NULL\n",
    "      AND ground_station_id IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    count_df = pd.read_sql(count_query, engine)\n",
    "    total_count = count_df['total_count'].iloc[0]\n",
    "    \n",
    "    print(f\"ðŸ“Š Total observations in last 4 years: {total_count:,}\")\n",
    "    \n",
    "    # Calculate sampling rate needed for 1M rows\n",
    "    sample_limit = 1000000\n",
    "    if total_count > sample_limit:\n",
    "        sampling_rate = sample_limit / total_count * 100\n",
    "        print(f\"ðŸ“Š Sampling {sample_limit:,} rows ({sampling_rate:.2f}% of total)\")\n",
    "    else:\n",
    "        sample_limit = total_count\n",
    "        print(f\"ðŸ“Š Will load all {total_count:,} observations\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error determining date range: {e}\")\n",
    "    raise SystemExit\n",
    "\n",
    "print(f\"â±ï¸  Date determination completed in {time.time() - start_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89ff30aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 3: EXTRACTING RANDOM SAMPLE FROM 4-YEAR PERIOD\n",
      "================================================================================\n",
      "Querying RANDOM sample of 1,000,000 observations from 2021-2025...\n",
      "Using ORDER BY RAND() for representative sampling across the entire period.\n",
      "Executing query: Random sample of 1,000,000 rows from 4-year period...\n",
      "âœ… Observations loaded: 1,000,000 rows\n",
      "â±ï¸  Load time: 2037.08s\n",
      "ðŸ“… Loaded data range: 2021-11-12 to 2025-11-12\n",
      "ðŸ“… Duration: 1461 days (4.0 years)\n",
      "\n",
      "ðŸ“Š Year distribution in sample:\n",
      "  2021:   31,157 rows (  3.12%)\n",
      "  2022:  225,327 rows ( 22.53%)\n",
      "  2023:  238,351 rows ( 23.84%)\n",
      "  2024:  262,490 rows ( 26.25%)\n",
      "  2025:  242,675 rows ( 24.27%)\n",
      "\n",
      "Observation data preview:\n",
      "Shape: (1000000, 16)\n",
      "Columns: ['id', 'start', 'end', 'status', 'waterfall_status', 'vetted_status', 'max_altitude', 'rise_azimuth', 'set_azimuth', 'ground_station_id', 'sat_id', 'archive_url', 'archived', 'experimental', 'client_version', 'year']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: EXTRACT RANDOM SAMPLE FROM 4-YEAR PERIOD\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: EXTRACTING RANDOM SAMPLE FROM 4-YEAR PERIOD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Querying RANDOM sample of {sample_limit:,} observations from 2021-2025...\")\n",
    "print(\"Using ORDER BY RAND() for representative sampling across the entire period.\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Query for RANDOM sample from 4-year period\n",
    "observation_query = f\"\"\"\n",
    "SELECT \n",
    "    -- Core observation data\n",
    "    id,\n",
    "    start,\n",
    "    end,\n",
    "    status,\n",
    "    waterfall_status,\n",
    "    vetted_status,\n",
    "    max_altitude,\n",
    "    rise_azimuth,\n",
    "    set_azimuth,\n",
    "    ground_station_id,\n",
    "    sat_id,\n",
    "    \n",
    "    -- Additional potentially useful fields\n",
    "    archive_url,\n",
    "    archived,\n",
    "    experimental,\n",
    "    client_version\n",
    "    \n",
    "FROM base_observation \n",
    "WHERE start >= '{cutoff_date.strftime('%Y-%m-%d')}'\n",
    "  AND max_altitude IS NOT NULL  -- Critical feature\n",
    "  AND status IS NOT NULL        -- Need status for target variable\n",
    "  AND ground_station_id IS NOT NULL  -- Need station ID for joining\n",
    "ORDER BY RAND()  -- KEY FIX: Random sampling for temporal distribution\n",
    "LIMIT {sample_limit}\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Executing query: Random sample of {sample_limit:,} rows from 4-year period...\")\n",
    "try:\n",
    "    obs_df = pd.read_sql(observation_query, engine)\n",
    "    print(f\"âœ… Observations loaded: {len(obs_df):,} rows\")\n",
    "    print(f\"â±ï¸  Load time: {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "    # Check date range of loaded data\n",
    "    if not obs_df.empty:\n",
    "        obs_df['start'] = pd.to_datetime(obs_df['start'])\n",
    "        min_date = obs_df['start'].min()\n",
    "        max_date = obs_df['start'].max()\n",
    "        date_range_days = (max_date - min_date).days\n",
    "        \n",
    "        print(f\"ðŸ“… Loaded data range: {min_date.strftime('%Y-%m-%d')} to {max_date.strftime('%Y-%m-%d')}\")\n",
    "        print(f\"ðŸ“… Duration: {date_range_days} days ({date_range_days/365:.1f} years)\")\n",
    "        \n",
    "        # Check year distribution\n",
    "        obs_df['year'] = obs_df['start'].dt.year\n",
    "        year_dist = obs_df['year'].value_counts().sort_index()\n",
    "        print(\"\\nðŸ“Š Year distribution in sample:\")\n",
    "        for year, count in year_dist.items():\n",
    "            percentage = count / len(obs_df) * 100\n",
    "            print(f\"  {year}: {count:8,} rows ({percentage:6.2f}%)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading observations: {e}\")\n",
    "    print(\"Trying alternative query...\")\n",
    "    \n",
    "    # Alternative: Use stratified sampling by year\n",
    "    year_sampling_query = f\"\"\"\n",
    "    WITH yearly_counts AS (\n",
    "        SELECT YEAR(start) as obs_year, COUNT(*) as year_count\n",
    "        FROM base_observation \n",
    "        WHERE start >= '{cutoff_date.strftime('%Y-%m-%d')}'\n",
    "          AND max_altitude IS NOT NULL\n",
    "          AND status IS NOT NULL\n",
    "          AND ground_station_id IS NOT NULL\n",
    "        GROUP BY YEAR(start)\n",
    "    )\n",
    "    SELECT o.*\n",
    "    FROM base_observation o\n",
    "    JOIN yearly_counts yc ON YEAR(o.start) = yc.obs_year\n",
    "    WHERE o.start >= '{cutoff_date.strftime('%Y-%m-%d')}'\n",
    "      AND o.max_altitude IS NOT NULL\n",
    "      AND o.status IS NOT NULL\n",
    "      AND o.ground_station_id IS NOT NULL\n",
    "      AND RAND() < {sample_limit} / (SELECT SUM(year_count) FROM yearly_counts)\n",
    "    LIMIT {sample_limit}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        obs_df = pd.read_sql(year_sampling_query, engine)\n",
    "        print(f\"âœ… Loaded stratified sample: {len(obs_df):,} rows\")\n",
    "    except Exception as e2:\n",
    "        print(f\"âŒ Failed to load data: {e2}\")\n",
    "        raise SystemExit\n",
    "\n",
    "print(\"\\nObservation data preview:\")\n",
    "print(f\"Shape: {obs_df.shape}\")\n",
    "print(f\"Columns: {list(obs_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cf806ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 4: LOADING STATION DATA\n",
      "================================================================================\n",
      "Loading ground station metadata...\n",
      "Unique ground_station_id values in our sample: 1556\n",
      "âœ… Stations loaded: 1,556 stations matching our observations\n",
      "â±ï¸  Load time: 0.12s\n",
      "\n",
      "ðŸ“Š Station matching statistics:\n",
      "  Observation station IDs requested: 1556\n",
      "  Station IDs found in database: 1556\n",
      "  Match rate: 100.0%\n",
      "\n",
      "Station data preview:\n",
      "Shape: (1556, 10)\n",
      "Columns: ['station_id', 'station_name', 'station_lat', 'station_lng', 'station_alt', 'horizon', 'grid_square', 'station_description', 'station_status', 'last_seen']\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: LOAD STATION DATA WITH BETTER FILTERING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: LOADING STATION DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Loading ground station metadata...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# First, let's check what ground_station_id values we have in our observation sample\n",
    "unique_ground_station_ids = obs_df['ground_station_id'].unique()\n",
    "print(f\"Unique ground_station_id values in our sample: {len(unique_ground_station_ids)}\")\n",
    "\n",
    "# Load ALL stations first, not just active ones\n",
    "station_query = f\"\"\"\n",
    "SELECT \n",
    "    id as station_id,\n",
    "    name as station_name,\n",
    "    lat as station_lat,\n",
    "    lng as station_lng,\n",
    "    alt as station_alt,\n",
    "    horizon,\n",
    "    qthlocator as grid_square,\n",
    "    description as station_description,\n",
    "    status as station_status,\n",
    "    last_seen\n",
    "FROM base_station\n",
    "WHERE id IN ({','.join([str(int(x)) for x in unique_ground_station_ids if not pd.isna(x)])})\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    station_df = pd.read_sql(station_query, engine)\n",
    "    print(f\"âœ… Stations loaded: {len(station_df):,} stations matching our observations\")\n",
    "    print(f\"â±ï¸  Load time: {time.time() - start_time:.2f}s\")\n",
    "    \n",
    "    # Check if we got all stations\n",
    "    matched_ids = set(station_df['station_id'].astype(int).tolist())\n",
    "    requested_ids = set([int(x) for x in unique_ground_station_ids if not pd.isna(x)])\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Station matching statistics:\")\n",
    "    print(f\"  Observation station IDs requested: {len(requested_ids)}\")\n",
    "    print(f\"  Station IDs found in database: {len(matched_ids)}\")\n",
    "    print(f\"  Match rate: {len(matched_ids)/len(requested_ids)*100:.1f}%\")\n",
    "    \n",
    "    if len(requested_ids) - len(matched_ids) > 0:\n",
    "        missing_ids = list(requested_ids - matched_ids)[:10]\n",
    "        print(f\"  Sample missing station IDs: {missing_ids}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Error loading specific stations: {e}\")\n",
    "    print(\"Loading all stations as fallback...\")\n",
    "    \n",
    "    # Fallback: Load all stations\n",
    "    station_query_all = \"\"\"\n",
    "    SELECT \n",
    "        id as station_id,\n",
    "        name as station_name,\n",
    "        lat as station_lat,\n",
    "        lng as station_lng,\n",
    "        alt as station_alt,\n",
    "        horizon,\n",
    "        qthlocator as grid_square,\n",
    "        description as station_description,\n",
    "        status as station_status,\n",
    "        last_seen\n",
    "    FROM base_station\n",
    "    WHERE lat IS NOT NULL AND lng IS NOT NULL\n",
    "    \"\"\"\n",
    "    \n",
    "    station_df = pd.read_sql(station_query_all, engine)\n",
    "    print(f\"âœ… Loaded all stations: {len(station_df):,} stations\")\n",
    "\n",
    "print(\"\\nStation data preview:\")\n",
    "print(f\"Shape: {station_df.shape}\")\n",
    "print(f\"Columns: {list(station_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6288d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 5: JOINING OBSERVATIONS WITH STATIONS\n",
      "================================================================================\n",
      "Before join: 1,000,000 observations\n",
      "Available stations: 1,556\n",
      "After join: 1,000,000 observations\n",
      "â±ï¸  Join completed in 1.29s\n",
      "\n",
      "ðŸ“Š Join statistics:\n",
      "  both           : 1,000,000 rows (100.00%)\n",
      "  left_only      :        0 rows (  0.00%)\n",
      "  right_only     :        0 rows (  0.00%)\n",
      "\n",
      "Expected join rate (based on station lookup): 100.0%\n",
      "Actual join rate: 100.0%\n",
      "\n",
      "Merged dataset shape: (1000000, 26)\n",
      "Columns in merged dataset: 26\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: JOIN OBSERVATIONS WITH STATION DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: JOINING OBSERVATIONS WITH STATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Before join: {len(obs_df):,} observations\")\n",
    "print(f\"Available stations: {len(station_df):,}\")\n",
    "\n",
    "# Ensure data types match for joining\n",
    "obs_df['ground_station_id'] = obs_df['ground_station_id'].astype(int)\n",
    "station_df['station_id'] = station_df['station_id'].astype(int)\n",
    "\n",
    "# Perform the join\n",
    "start_time = time.time()\n",
    "\n",
    "df_merged = obs_df.merge(\n",
    "    station_df,\n",
    "    left_on='ground_station_id',\n",
    "    right_on='station_id',\n",
    "    how='left',  # Keep all observations even if station not found\n",
    "    indicator=True  # Track join status\n",
    ")\n",
    "\n",
    "print(f\"After join: {len(df_merged):,} observations\")\n",
    "print(f\"â±ï¸  Join completed in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Analyze join quality\n",
    "join_stats = df_merged['_merge'].value_counts()\n",
    "print(\"\\nðŸ“Š Join statistics:\")\n",
    "for merge_type, count in join_stats.items():\n",
    "    percentage = count / len(df_merged) * 100\n",
    "    print(f\"  {merge_type:15}: {count:8,} rows ({percentage:6.2f}%)\")\n",
    "\n",
    "# Calculate expected vs actual join rate\n",
    "expected_join_rate = len(station_df) / len(unique_ground_station_ids) * 100\n",
    "print(f\"\\nExpected join rate (based on station lookup): {expected_join_rate:.1f}%\")\n",
    "print(f\"Actual join rate: {join_stats.get('both', 0)/len(df_merged)*100:.1f}%\")\n",
    "\n",
    "# Drop the merge indicator column\n",
    "df_merged = df_merged.drop(columns=['_merge'])\n",
    "\n",
    "print(f\"\\nMerged dataset shape: {df_merged.shape}\")\n",
    "print(f\"Columns in merged dataset: {len(df_merged.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e7ea6c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 6: DEFINING TARGET VARIABLE\n",
      "================================================================================\n",
      "Analyzing status codes in 4-year data...\n",
      "\n",
      "Top 10 status codes in 4-year data:\n",
      "  Status    100:  421,988 rows ( 42.20%)\n",
      "  Status      0:  285,876 rows ( 28.59%)\n",
      "  Status   -100:  202,384 rows ( 20.24%)\n",
      "  Status  -1000:   89,752 rows (  8.98%)\n",
      "\n",
      "ðŸŽ¯ Target variable distribution:\n",
      "  Ambiguous (will be filtered out):  421,988 rows ( 42.20%)\n",
      "  Failure                       :  292,136 rows ( 29.21%)\n",
      "  Success                       :  285,876 rows ( 28.59%)\n",
      "\n",
      "ðŸ“ˆ Success rate (excluding ambiguous): 49.46%\n",
      "   (285,876 successes / 578,012 valid observations)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: DEFINE TARGET VARIABLE\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: DEFINING TARGET VARIABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Analyzing status codes in 4-year data...\")\n",
    "status_dist = df_merged['status'].value_counts().head(10)\n",
    "print(\"\\nTop 10 status codes in 4-year data:\")\n",
    "for status_code, count in status_dist.items():\n",
    "    percentage = count / len(df_merged) * 100\n",
    "    print(f\"  Status {status_code:6}: {count:8,} rows ({percentage:6.2f}%)\")\n",
    "\n",
    "# Create target mapping\n",
    "def map_status_to_target(status):\n",
    "    \"\"\"Map status code to target variable.\"\"\"\n",
    "    if pd.isna(status):\n",
    "        return np.nan\n",
    "    \n",
    "    status = int(status) if not pd.isna(status) else status\n",
    "    \n",
    "    # Success cases\n",
    "    if status == 0:\n",
    "        return 1  # Success\n",
    "    \n",
    "    # Failure cases\n",
    "    elif status in [-100, -1000]:\n",
    "        return 0  # Failure\n",
    "    \n",
    "    # Ambiguous/unknown cases\n",
    "    else:\n",
    "        return np.nan  # Will filter these out\n",
    "\n",
    "# Create binary target\n",
    "df_merged['target_success'] = df_merged['status'].apply(map_status_to_target)\n",
    "\n",
    "# Also create detailed status category for analysis\n",
    "def map_status_to_category(status):\n",
    "    \"\"\"Map status code to descriptive category.\"\"\"\n",
    "    if pd.isna(status):\n",
    "        return 'unknown'\n",
    "    \n",
    "    status = int(status) if not pd.isna(status) else status\n",
    "    \n",
    "    if status == 0:\n",
    "        return 'success'\n",
    "    elif status == -100:\n",
    "        return 'failure'\n",
    "    elif status == -1000:\n",
    "        return 'severe_failure'\n",
    "    elif status == 100:\n",
    "        return 'in_progress'\n",
    "    elif status > 0:\n",
    "        return f'positive_{status}'\n",
    "    else:\n",
    "        return f'negative_{abs(status)}'\n",
    "\n",
    "df_merged['status_category'] = df_merged['status'].apply(map_status_to_category)\n",
    "\n",
    "# Analyze target distribution\n",
    "print(\"\\nðŸŽ¯ Target variable distribution:\")\n",
    "target_dist = df_merged['target_success'].value_counts(dropna=False)\n",
    "\n",
    "total_obs = len(df_merged)\n",
    "for target_val, count in target_dist.items():\n",
    "    if pd.isna(target_val):\n",
    "        label = 'Ambiguous (will be filtered out)'\n",
    "    else:\n",
    "        label = 'Success' if target_val == 1 else 'Failure'\n",
    "    \n",
    "    percentage = count / total_obs * 100\n",
    "    print(f\"  {label:30}: {count:8,} rows ({percentage:6.2f}%)\")\n",
    "\n",
    "# Calculate overall success rate\n",
    "success_count = target_dist.get(1, 0)\n",
    "failure_count = target_dist.get(0, 0)\n",
    "valid_obs = success_count + failure_count\n",
    "\n",
    "if valid_obs > 0:\n",
    "    success_rate = success_count / valid_obs * 100\n",
    "    print(f\"\\nðŸ“ˆ Success rate (excluding ambiguous): {success_rate:.2f}%\")\n",
    "    print(f\"   ({success_count:,} successes / {valid_obs:,} valid observations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f1cbd7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 7: CREATING TEMPORAL FEATURES\n",
      "================================================================================\n",
      "Converting timestamps and creating temporal features...\n",
      "âœ… Temporal features created in 3.24s\n",
      "\n",
      "ðŸ“… Temporal distribution:\n",
      "   - Date range: 2021-11-12 to 2025-11-12\n",
      "   - Years: [np.int32(2021), np.int32(2022), np.int32(2023), np.int32(2024), np.int32(2025)]\n",
      "   - Months: [np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6), np.int32(7), np.int32(8), np.int32(9), np.int32(10), np.int32(11), np.int32(12)]\n",
      "   - Seasons: ['fall', 'spring', 'summer', 'winter']\n",
      "   - Time of day: ['afternoon', 'evening', 'morning', 'night']\n",
      "âœ… All seasons represented in data\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: CREATE TEMPORAL FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 7: CREATING TEMPORAL FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Converting timestamps and creating temporal features...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Convert timestamps\n",
    "df_merged['start'] = pd.to_datetime(df_merged['start'], errors='coerce')\n",
    "df_merged['end'] = pd.to_datetime(df_merged['end'], errors='coerce')\n",
    "\n",
    "# Calculate duration in seconds\n",
    "df_merged['duration_seconds'] = (df_merged['end'] - df_merged['start']).dt.total_seconds()\n",
    "\n",
    "# Extract time components\n",
    "df_merged['hour_of_day'] = df_merged['start'].dt.hour\n",
    "df_merged['day_of_week'] = df_merged['start'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df_merged['month'] = df_merged['start'].dt.month\n",
    "df_merged['year'] = df_merged['start'].dt.year\n",
    "df_merged['day_of_year'] = df_merged['start'].dt.dayofyear\n",
    "df_merged['week_of_year'] = df_merged['start'].dt.isocalendar().week\n",
    "\n",
    "# Create time of day categories\n",
    "def categorize_time_of_day(hour):\n",
    "    \"\"\"Categorize hour into time of day periods.\"\"\"\n",
    "    if pd.isna(hour):\n",
    "        return 'unknown'\n",
    "    elif 0 <= hour < 6:\n",
    "        return 'night'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'evening'\n",
    "\n",
    "df_merged['time_of_day'] = df_merged['hour_of_day'].apply(categorize_time_of_day)\n",
    "\n",
    "# Create season based on month\n",
    "def get_season(month):\n",
    "    \"\"\"Convert month to season (northern hemisphere).\"\"\"\n",
    "    if pd.isna(month):\n",
    "        return 'unknown'\n",
    "    elif month in [12, 1, 2]:\n",
    "        return 'winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'summer'\n",
    "    else:\n",
    "        return 'fall'\n",
    "\n",
    "df_merged['season'] = df_merged['month'].apply(get_season)\n",
    "\n",
    "print(f\"âœ… Temporal features created in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Display temporal distribution\n",
    "print(f\"\\nðŸ“… Temporal distribution:\")\n",
    "print(f\"   - Date range: {df_merged['start'].min().strftime('%Y-%m-%d')} to {df_merged['start'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"   - Years: {sorted(df_merged['year'].unique())}\")\n",
    "print(f\"   - Months: {sorted(df_merged['month'].unique())}\")\n",
    "print(f\"   - Seasons: {sorted(df_merged['season'].unique())}\")\n",
    "print(f\"   - Time of day: {sorted(df_merged['time_of_day'].unique())}\")\n",
    "\n",
    "# Check if we have all seasons\n",
    "expected_seasons = ['winter', 'spring', 'summer', 'fall']\n",
    "actual_seasons = df_merged['season'].unique()\n",
    "missing_seasons = set(expected_seasons) - set(actual_seasons)\n",
    "\n",
    "if missing_seasons:\n",
    "    print(f\"âš ï¸  Missing seasons: {missing_seasons}\")\n",
    "else:\n",
    "    print(\"âœ… All seasons represented in data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16cc08f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 8: CREATING GEOMETRIC FEATURES\n",
      "================================================================================\n",
      "Creating geometric features from pass parameters...\n",
      "âœ… Geometric features created:\n",
      "   - Elevation categories: ['high', 'low', 'medium', 'very_high', 'very_low']\n",
      "   - Duration categories: ['long', 'medium', 'short', 'very_long']\n",
      "   - Elevation range: 0.0 to 90.0 degrees\n",
      "   - Duration range: 180 to 25322 seconds\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 8: CREATE GEOMETRIC FEATURES\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 8: CREATING GEOMETRIC FEATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"Creating geometric features from pass parameters...\")\n",
    "\n",
    "# Elevation categories (max_altitude in degrees)\n",
    "def categorize_elevation(altitude):\n",
    "    \"\"\"Categorize maximum altitude into bins.\"\"\"\n",
    "    if pd.isna(altitude):\n",
    "        return 'unknown'\n",
    "    elif altitude < 10:\n",
    "        return 'very_low'\n",
    "    elif altitude < 30:\n",
    "        return 'low'\n",
    "    elif altitude < 60:\n",
    "        return 'medium'\n",
    "    elif altitude < 85:\n",
    "        return 'high'\n",
    "    else:\n",
    "        return 'very_high'\n",
    "\n",
    "df_merged['elevation_category'] = df_merged['max_altitude'].apply(categorize_elevation)\n",
    "\n",
    "# Duration categories\n",
    "def categorize_duration(duration):\n",
    "    \"\"\"Categorize observation duration.\"\"\"\n",
    "    if pd.isna(duration):\n",
    "        return 'unknown'\n",
    "    elif duration < 60:  # Less than 1 minute\n",
    "        return 'very_short'\n",
    "    elif duration < 300:  # Less than 5 minutes\n",
    "        return 'short'\n",
    "    elif duration < 900:  # Less than 15 minutes\n",
    "        return 'medium'\n",
    "    elif duration < 1800:  # Less than 30 minutes\n",
    "        return 'long'\n",
    "    else:\n",
    "        return 'very_long'\n",
    "\n",
    "df_merged['duration_category'] = df_merged['duration_seconds'].apply(categorize_duration)\n",
    "\n",
    "# Azimuth range (how much the satellite moves in azimuth)\n",
    "df_merged['azimuth_range'] = abs(df_merged['set_azimuth'] - df_merged['rise_azimuth'])\n",
    "# Normalize to 0-360 range\n",
    "df_merged['azimuth_range'] = df_merged['azimuth_range'].apply(lambda x: min(x, 360-x) if not pd.isna(x) else np.nan)\n",
    "\n",
    "print(\"âœ… Geometric features created:\")\n",
    "print(f\"   - Elevation categories: {sorted(df_merged['elevation_category'].unique())}\")\n",
    "print(f\"   - Duration categories: {sorted(df_merged['duration_category'].unique())}\")\n",
    "print(f\"   - Elevation range: {df_merged['max_altitude'].min():.1f} to {df_merged['max_altitude'].max():.1f} degrees\")\n",
    "print(f\"   - Duration range: {df_merged['duration_seconds'].min():.0f} to {df_merged['duration_seconds'].max():.0f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ee0ee85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 9: DATA CLEANING AND VALIDATION\n",
      "================================================================================\n",
      "Initial rows: 1,000,000\n",
      "1. After removing ambiguous status: 578,012 rows (removed 421,988)\n",
      "2. After removing invalid durations: 578,010 rows (removed 2)\n",
      "3. After removing invalid elevations: 578,010 rows (removed 0)\n",
      "4. After removing duplicates: 578,010 rows (removed 0)\n",
      "\n",
      "ðŸ” Missing values in critical columns:\n",
      "  âœ… max_altitude        : No missing values\n",
      "  âœ… duration_seconds    : No missing values\n",
      "  âœ… target_success      : No missing values\n",
      "  âœ… hour_of_day         : No missing values\n",
      "  âœ… station_lat         : No missing values\n",
      "\n",
      "ðŸ“Š Final dataset size: 578,010 observations\n",
      "ðŸ“ˆ Success rate in cleaned data: 49.46%\n",
      "ðŸ“ Observations with station coordinates: 578,010 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: DATA CLEANING AND VALIDATION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 9: DATA CLEANING AND VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "initial_rows = len(df_merged)\n",
    "print(f\"Initial rows: {initial_rows:,}\")\n",
    "\n",
    "# 1. Filter out ambiguous observations (no clear success/failure)\n",
    "df_clean = df_merged[df_merged['target_success'].notnull()].copy()\n",
    "ambiguous_removed = initial_rows - len(df_clean)\n",
    "print(f\"1. After removing ambiguous status: {len(df_clean):,} rows (removed {ambiguous_removed:,})\")\n",
    "\n",
    "# 2. Remove observations with invalid durations\n",
    "valid_duration = (df_clean['duration_seconds'] > 10) & (df_clean['duration_seconds'] < 7200)  # 10s to 2 hours\n",
    "invalid_duration_count = (~valid_duration).sum()\n",
    "df_clean = df_clean[valid_duration].copy()\n",
    "print(f\"2. After removing invalid durations: {len(df_clean):,} rows (removed {invalid_duration_count:,})\")\n",
    "\n",
    "# 3. Remove invalid elevations (should be between -90 and 90 degrees)\n",
    "valid_elevation = (df_clean['max_altitude'] >= -90) & (df_clean['max_altitude'] <= 90)\n",
    "invalid_elevation_count = (~valid_elevation).sum()\n",
    "df_clean = df_clean[valid_elevation].copy()\n",
    "print(f\"3. After removing invalid elevations: {len(df_clean):,} rows (removed {invalid_elevation_count:,})\")\n",
    "\n",
    "# 4. Remove duplicate observations if any\n",
    "initial_len = len(df_clean)\n",
    "df_clean = df_clean.drop_duplicates(subset=['id'], keep='first')\n",
    "duplicates_removed = initial_len - len(df_clean)\n",
    "print(f\"4. After removing duplicates: {len(df_clean):,} rows (removed {duplicates_removed})\")\n",
    "\n",
    "# Check for remaining missing values in critical columns\n",
    "print(\"\\nðŸ” Missing values in critical columns:\")\n",
    "critical_columns = ['max_altitude', 'duration_seconds', 'target_success', 'hour_of_day', 'station_lat']\n",
    "missing_summary = []\n",
    "\n",
    "for col in critical_columns:\n",
    "    if col in df_clean.columns:\n",
    "        null_count = df_clean[col].isnull().sum()\n",
    "        null_pct = null_count / len(df_clean) * 100\n",
    "        missing_summary.append((col, null_count, null_pct))\n",
    "        \n",
    "        if null_count > 0:\n",
    "            print(f\"  âš ï¸  {col:20}: {null_count:6,} NULL ({null_pct:5.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  âœ… {col:20}: No missing values\")\n",
    "    else:\n",
    "        print(f\"  âŒ {col:20}: COLUMN NOT FOUND\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Final dataset size: {len(df_clean):,} observations\")\n",
    "print(f\"ðŸ“ˆ Success rate in cleaned data: {df_clean['target_success'].mean() * 100:.2f}%\")\n",
    "\n",
    "# Analyze station data availability\n",
    "if 'station_lat' in df_clean.columns:\n",
    "    station_data_available = df_clean['station_lat'].notnull().sum()\n",
    "    station_data_pct = station_data_available / len(df_clean) * 100\n",
    "    print(f\"ðŸ“ Observations with station coordinates: {station_data_available:,} ({station_data_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6225ea37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 10: FEATURE ANALYSIS\n",
      "================================================================================\n",
      "ðŸ“‹ Feature inventory by category:\n",
      "------------------------------------------------------------\n",
      "\n",
      "Temporal Features (10 features):\n",
      "  â€¢ hour_of_day               - int32           -    24 unique values\n",
      "  â€¢ day_of_week               - int32           -     7 unique values\n",
      "  â€¢ month                     - int32           -    12 unique values\n",
      "  â€¢ year                      - int32           -     5 unique values\n",
      "  â€¢ season                    - object          -     4 unique values\n",
      "  â€¢ time_of_day               - object          -     4 unique values\n",
      "  â€¢ duration_seconds          - float64         -  1767 unique values\n",
      "  â€¢ duration_category         - object          -     4 unique values\n",
      "  â€¢ week_of_year              - UInt32          -    52 unique values\n",
      "  â€¢ day_of_year               - int32           -   366 unique values\n",
      "\n",
      "Geometric Features (5 features):\n",
      "  â€¢ max_altitude              - float64         -    91 unique values\n",
      "  â€¢ rise_azimuth              - float64         -   361 unique values\n",
      "  â€¢ set_azimuth               - float64         -   361 unique values\n",
      "  â€¢ elevation_category        - object          -     5 unique values\n",
      "  â€¢ azimuth_range             - float64         -   181 unique values\n",
      "\n",
      "Station Features (7 features):\n",
      "  â€¢ station_lat               - float64         -  1307 unique values\n",
      "  â€¢ station_lng               - float64         -  1311 unique values\n",
      "  â€¢ station_alt               - int64           -   451 unique values\n",
      "  â€¢ horizon                   - int64           -    42 unique values\n",
      "  â€¢ grid_square               - object          -  1133 unique values\n",
      "  â€¢ station_status            - int64           -     3 unique values\n",
      "  â€¢ station_name              - object          -  1511 unique values\n",
      "\n",
      "Target Variables (4 features):\n",
      "  â€¢ target_success            - float64         -     2 unique values\n",
      "  â€¢ status                    - int64           -     3 unique values\n",
      "  â€¢ status_category           - object          -     3 unique values\n",
      "  â€¢ vetted_status             - object          -     1 unique values\n",
      "\n",
      "Satellite Info (1 features):\n",
      "  â€¢ sat_id                    - object          -  2031 unique values\n",
      "\n",
      "Metadata (10 features):\n",
      "  â€¢ id                        - int64           - 578010 unique values\n",
      "  â€¢ start                     - datetime64[ns]  - 573880 unique values\n",
      "  â€¢ end                       - datetime64[ns]  - 573747 unique values\n",
      "  â€¢ ground_station_id         - int64           -  1526 unique values\n",
      "  â€¢ station_id                - int64           -  1526 unique values\n",
      "  â€¢ waterfall_status          - float64         -     1 unique values\n",
      "  â€¢ archive_url               - object          - 354433 unique values\n",
      "  â€¢ client_version            - object          -   164 unique values\n",
      "  â€¢ archived                  - int64           -     2 unique values\n",
      "  â€¢ experimental              - int64           -     2 unique values\n",
      "\n",
      "ðŸ“Š Total features available: 37\n",
      "\n",
      "ðŸ”— Correlation of top numeric features with target:\n",
      "  status                   :   0.538 (Strong)\n",
      "  archived                 :   0.117 (Moderate)\n",
      "  experimental             :  -0.099 (Weak)\n",
      "  year                     :   0.083 (Weak)\n",
      "  max_altitude             :   0.071 (Weak)\n",
      "  rise_azimuth             :  -0.064 (Weak)\n",
      "  set_azimuth              :   0.053 (Weak)\n",
      "  station_status           :   0.038 (Weak)\n",
      "  horizon                  :   0.023 (Weak)\n",
      "  station_lat              :   0.013 (Weak)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: FEATURE ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 10: FEATURE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group features by category\n",
    "feature_categories = {\n",
    "    'Temporal Features': ['hour_of_day', 'day_of_week', 'month', 'year', \n",
    "                         'season', 'time_of_day', 'duration_seconds', 'duration_category',\n",
    "                         'week_of_year', 'day_of_year'],\n",
    "    'Geometric Features': ['max_altitude', 'rise_azimuth', 'set_azimuth', \n",
    "                          'elevation_category', 'azimuth_range'],\n",
    "    'Station Features': ['station_lat', 'station_lng', 'station_alt', 'horizon', \n",
    "                        'grid_square', 'station_status', 'station_name'],\n",
    "    'Target Variables': ['target_success', 'status', 'status_category', 'vetted_status'],\n",
    "    'Satellite Info': ['sat_id'],\n",
    "    'Metadata': ['id', 'start', 'end', 'ground_station_id', 'station_id',\n",
    "                'waterfall_status', 'archive_url', 'client_version', 'archived', 'experimental']\n",
    "}\n",
    "\n",
    "print(\"ðŸ“‹ Feature inventory by category:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "available_features = []\n",
    "for category, features in feature_categories.items():\n",
    "    existing_features = [f for f in features if f in df_clean.columns]\n",
    "    if existing_features:\n",
    "        print(f\"\\n{category} ({len(existing_features)} features):\")\n",
    "        for feature in existing_features:\n",
    "            dtype = str(df_clean[feature].dtype)\n",
    "            unique_count = df_clean[feature].nunique()\n",
    "            available_features.append(feature)\n",
    "            print(f\"  â€¢ {feature:25} - {dtype:15} - {unique_count:5} unique values\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Total features available: {len(available_features)}\")\n",
    "\n",
    "# Quick correlation analysis\n",
    "if 'target_success' in df_clean.columns:\n",
    "    print(\"\\nðŸ”— Correlation of top numeric features with target:\")\n",
    "    numeric_features = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Remove target and ID columns\n",
    "    exclude_cols = ['target_success', 'id', 'ground_station_id', 'station_id']\n",
    "    numeric_features = [f for f in numeric_features if f not in exclude_cols]\n",
    "    \n",
    "    correlations = []\n",
    "    for feature in numeric_features[:15]:  # Check first 15 numeric features\n",
    "        if df_clean[feature].notnull().sum() > len(df_clean) * 0.5:  # At least 50% non-null\n",
    "            corr = df_clean[feature].corr(df_clean['target_success'])\n",
    "            if not pd.isna(corr):\n",
    "                correlations.append((feature, corr))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    correlations.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "    \n",
    "    for feature, corr in correlations[:10]:  # Top 10 correlations\n",
    "        correlation_strength = \"Strong\" if abs(corr) > 0.3 else \"Moderate\" if abs(corr) > 0.1 else \"Weak\"\n",
    "        print(f\"  {feature:25}: {corr:7.3f} ({correlation_strength})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ce5ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 11: SAVING PROCESSED DATASET\n",
      "================================================================================\n",
      "ðŸ’¾ Saving dataset to: ../1_datasets/processed\\four_year_observations_20251211_1730.csv\n",
      "âœ… Dataset saved successfully!\n",
      "   - Rows: 578,010\n",
      "   - Columns: 39\n",
      "   - File size: 261.64 MB\n",
      "   - Save time: 24.29s\n",
      "\n",
      "ðŸ’¾ Sample dataset saved: ../1_datasets/processed\\four_year_sample_50000.csv\n",
      "   - Sample size: 50,000 rows\n",
      "   - File size: 22.64 MB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 11: SAVE PROCESSED DATASET\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 11: SAVING PROCESSED DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directories\n",
    "output_dir = \"../1_datasets/processed\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Generate filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "filename = f\"four_year_observations_{timestamp}.csv\"\n",
    "full_path = os.path.join(output_dir, filename)\n",
    "\n",
    "# Save the full dataset\n",
    "print(f\"ðŸ’¾ Saving dataset to: {full_path}\")\n",
    "start_time = time.time()\n",
    "\n",
    "df_clean.to_csv(full_path, index=False)\n",
    "\n",
    "save_time = time.time() - start_time\n",
    "file_size_mb = os.path.getsize(full_path) / (1024 ** 2)\n",
    "\n",
    "print(f\"âœ… Dataset saved successfully!\")\n",
    "print(f\"   - Rows: {len(df_clean):,}\")\n",
    "print(f\"   - Columns: {df_clean.shape[1]}\")\n",
    "print(f\"   - File size: {file_size_mb:.2f} MB\")\n",
    "print(f\"   - Save time: {save_time:.2f}s\")\n",
    "\n",
    "# Also save a smaller sample for quick EDA\n",
    "sample_size = min(50000, len(df_clean))\n",
    "df_sample = df_clean.sample(n=sample_size, random_state=42, replace=False)\n",
    "sample_path = os.path.join(output_dir, f\"four_year_sample_{sample_size}.csv\")\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "\n",
    "print(f\"\\nðŸ’¾ Sample dataset saved: {sample_path}\")\n",
    "print(f\"   - Sample size: {len(df_sample):,} rows\")\n",
    "print(f\"   - File size: {os.path.getsize(sample_path) / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c262bd5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STEP 12: CREATING SUMMARY REPORT\n",
      "================================================================================\n",
      "ðŸ“„ Summary report saved: ../1_datasets/processed\\four_year_summary_20251211_1730.md\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ KEY ACHIEVEMENTS\n",
      "================================================================================\n",
      "âœ… 4-Year Coverage: 2021 to 2025\n",
      "âœ… Seasonal Representation: ['fall', 'spring', 'summer', 'winter']\n",
      "âœ… Observations: 578,010 clean rows\n",
      "âœ… Success Rate: 49.46%\n",
      "âœ… Features Created: 39 total columns\n",
      "âœ… Station Coverage: 100.0% with coordinates\n",
      "\n",
      "================================================================================\n",
      "ðŸ“‹ DATASET PREVIEW (Rows with station data)\n",
      "================================================================================\n",
      "                start  duration_seconds  max_altitude elevation_category  hour_of_day  station_lat  station_lng  target_success\n",
      "1 2025-10-18 09:45:18             317.0          29.0                low            9     53.91000     27.09000             1.0\n",
      "4 2025-09-04 12:31:04             180.0          46.0             medium           12     40.16400   -105.09300             0.0\n",
      "5 2022-08-24 22:39:08             759.0          78.0               high           22     47.25833     16.60462             0.0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 12: CREATE COMPREHENSIVE SUMMARY REPORT\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 12: CREATING SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate additional statistics\n",
    "total_years = df_clean['year'].nunique()\n",
    "total_months = df_clean['month'].nunique()\n",
    "total_seasons = df_clean['season'].nunique()\n",
    "station_coverage = df_clean['station_lat'].notnull().sum() / len(df_clean) * 100\n",
    "\n",
    "# Generate comprehensive summary\n",
    "summary = f\"\"\"\n",
    "# FOUR-YEAR OBSERVATION DATASET SUMMARY (2021-2025)\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Filename: {filename}\n",
    "\n",
    "## DATA EXTRACTION\n",
    "- Time period requested: Last 4 years (from {cutoff_date.strftime('%Y-%m-%d')})\n",
    "- Latest observation in DB: {latest_date.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- Observations in 4-year period: {total_count:,}\n",
    "- Sample size extracted: {sample_limit:,}\n",
    "- Final cleaned observations: {len(df_clean):,}\n",
    "\n",
    "## TEMPORAL COVERAGE\n",
    "- Date range: {df_clean['start'].min().strftime('%Y-%m-%d')} to {df_clean['start'].max().strftime('%Y-%m-%d')}\n",
    "- Years represented: {sorted(df_clean['year'].unique())} ({total_years} years)\n",
    "- Months represented: {sorted(df_clean['month'].unique())} ({total_months} months)\n",
    "- Seasons represented: {sorted(df_clean['season'].unique())} ({total_seasons} seasons)\n",
    "- Total time span: {(df_clean['start'].max() - df_clean['start'].min()).days} days\n",
    "\n",
    "## TARGET VARIABLE\n",
    "- Success (1): {df_clean['target_success'].sum():,} observations\n",
    "- Failure (0): {len(df_clean) - df_clean['target_success'].sum():,} observations\n",
    "- Success rate: {df_clean['target_success'].mean() * 100:.2f}%\n",
    "- Ambiguous observations removed: {ambiguous_removed:,}\n",
    "\n",
    "## STATUS CODE DISTRIBUTION (Original)\n",
    "{status_dist.head(10).to_string()}\n",
    "\n",
    "## STATION DATA COVERAGE\n",
    "- Observations with station coordinates: {df_clean['station_lat'].notnull().sum():,}\n",
    "- Station coverage: {station_coverage:.1f}%\n",
    "- Unique stations: {df_clean['ground_station_id'].nunique():,}\n",
    "- Unique stations with location data: {df_clean[df_clean['station_lat'].notnull()]['ground_station_id'].nunique():,}\n",
    "\n",
    "## FEATURE CATEGORIES\n",
    "### Temporal Features ({len([f for f in feature_categories['Temporal Features'] if f in df_clean.columns])})\n",
    "- Hour, day, week, month, year\n",
    "- Season, time of day categories\n",
    "- Duration in seconds and categories\n",
    "\n",
    "### Geometric Features ({len([f for f in feature_categories['Geometric Features'] if f in df_clean.columns])})\n",
    "- Maximum altitude: {df_clean['max_altitude'].min():.1f} to {df_clean['max_altitude'].max():.1f} degrees\n",
    "- Elevation categories: {sorted(df_clean['elevation_category'].unique())}\n",
    "- Duration range: {df_clean['duration_seconds'].min():.0f} to {df_clean['duration_seconds'].max():.0f} seconds\n",
    "\n",
    "### Station Features ({len([f for f in feature_categories['Station Features'] if f in df_clean.columns])})\n",
    "- Latitude, longitude, altitude\n",
    "- Horizon minimum elevation\n",
    "- Grid square location\n",
    "- Station status\n",
    "\n",
    "## DATA QUALITY\n",
    "- Invalid durations removed: {invalid_duration_count:,}\n",
    "- Invalid elevations removed: {invalid_elevation_count:,}\n",
    "- Duplicates removed: {duplicates_removed:,}\n",
    "- Missing station coordinates: {(len(df_clean) - df_clean['station_lat'].notnull().sum()):,}\n",
    "\n",
    "## FEATURE CORRELATIONS WITH SUCCESS\n",
    "Top correlations:\n",
    "{chr(10).join([f\"- {feat}: {corr:.3f}\" for feat, corr in correlations[:5]])}\n",
    "\n",
    "## FILES CREATED\n",
    "1. `{filename}` - Full cleaned dataset ({len(df_clean):,} rows, {file_size_mb:.1f} MB)\n",
    "2. `four_year_sample_{sample_size}.csv` - EDA sample ({len(df_sample):,} rows)\n",
    "\n",
    "## NEXT STEPS\n",
    "1. Explore dataset in `3_data_exploration/` \n",
    "2. Engineer additional features (satellite metadata, weather, etc.)\n",
    "3. Address missing station data (imputation or feature engineering)\n",
    "4. Create train/test splits for modeling\n",
    "5. Begin model development in `4_data_analysis/`\n",
    "\"\"\"\n",
    "\n",
    "# Save summary to file\n",
    "summary_path = os.path.join(output_dir, f\"four_year_summary_{timestamp}.md\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    f.write(summary)\n",
    "\n",
    "print(f\"ðŸ“„ Summary report saved: {summary_path}\")\n",
    "\n",
    "# Display key statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸŽ¯ KEY ACHIEVEMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"âœ… 4-Year Coverage: {df_clean['year'].min()} to {df_clean['year'].max()}\")\n",
    "print(f\"âœ… Seasonal Representation: {sorted(df_clean['season'].unique())}\")\n",
    "print(f\"âœ… Observations: {len(df_clean):,} clean rows\")\n",
    "print(f\"âœ… Success Rate: {df_clean['target_success'].mean() * 100:.2f}%\")\n",
    "print(f\"âœ… Features Created: {df_clean.shape[1]} total columns\")\n",
    "print(f\"âœ… Station Coverage: {station_coverage:.1f}% with coordinates\")\n",
    "\n",
    "# Show sample of data with station info\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“‹ DATASET PREVIEW (Rows with station data)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show rows that have station data\n",
    "if 'station_lat' in df_clean.columns and df_clean['station_lat'].notnull().sum() > 0:\n",
    "    station_rows = df_clean[df_clean['station_lat'].notnull()].head(3)\n",
    "    preview_cols = ['start', 'duration_seconds', 'max_altitude', 'elevation_category', \n",
    "                   'hour_of_day', 'station_lat', 'station_lng', 'target_success']\n",
    "    available_preview_cols = [col for col in preview_cols if col in station_rows.columns]\n",
    "    \n",
    "    if available_preview_cols:\n",
    "        print(station_rows[available_preview_cols].head(3).to_string())\n",
    "else:\n",
    "    print(\"No station data available in sample\")\n",
    "    print(df_clean[['start', 'duration_seconds', 'max_altitude', 'target_success']].head(3).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5b98483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… FOUR-YEAR DATASET PREPARATION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "â±ï¸  Total execution time: 7200.00 seconds\n",
      "\n",
      "ðŸŽ¯ Next Steps:\n",
      "   1. Review dataset: 1_datasets/processed/\n",
      "   2. Check temporal coverage in summary report\n",
      "   3. Proceed to EDA: 3_data_exploration/\n",
      "\n",
      "ðŸ“ Your 4-year dataset is ready at: ../1_datasets/processed\\four_year_observations_20251211_1730.csv\n",
      "\n",
      "================================================================================\n",
      "ðŸš€ Ready for Exploration & Modeling!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# COMPLETION\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… FOUR-YEAR DATASET PREPARATION COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "total_time = time.time() - pd.to_datetime('now').timestamp()\n",
    "print(f\"\\nâ±ï¸  Total execution time: {abs(total_time):.2f} seconds\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"   1. Review dataset: 1_datasets/processed/\")\n",
    "print(\"   2. Check temporal coverage in summary report\")\n",
    "print(\"   3. Proceed to EDA: 3_data_exploration/\")\n",
    "\n",
    "print(f\"\\nðŸ“ Your 4-year dataset is ready at: {full_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸš€ Ready for Exploration & Modeling!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
